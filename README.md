# Roboprof
<div style="display: flex; justify-content: center;">
    <img src="https://github.com/vidhisagathiya/Roboprof/blob/main/RoboProf.png" alt="Roboprof" style="width: 45%;">
    <img src="https://github.com/vidhisagathiya/Roboprof/blob/main/Project_Structure.png" alt="Project Structure" style="width: 45%;">
</div>

## Introduction

Roboprof is an intelligent agent designed to answer university course- and student-related inquiries using a knowledge graph and natural language processing techniques. This project focuses on constructing a knowledge base about Concordia University's courses by leveraging open datasets available at [Concordia Open Data Portal](https://opendata.concordia.ca/datasets/).

## Purpose of Each File

1. **CATALOG_2023_09_19.csv** and **CU_SR_OPEN_DATA_CATALOG_UTF8.csv**: CSV files obtained from the Concordia University's open data portal. These datasets contain information about Concordia's courses, including course numbers, codes, names, descriptions, and other relevant details.

2. **CLEANED_DATA.csv**: The cleaned and merged dataset containing information about Concordia's courses. This file is generated by mergeCSV.py and serves as input for main.py.

3. **requirements.txt**: A text file containing the required Python libraries and their versions for running the project.

4. **vocabulary.ttl**: A Turtle file containing the vocabulary schema for the knowledge base. It defines classes and properties used in the knowledge base construction.

5. **mergeCSV.py**: Python script used for cleaning and merging CSV files containing course data. It ensures the dataset is properly formatted before being utilized by the main program.
   
6. **main.py**: The main Python script responsible for generating the knowledge base. It utilizes provided datasets to populate the knowledge base with comprehensive information about Concordia's courses and their related materials.

7. **kb.ttl**: The Turtle file representing the knowledge base. It contains triples generated by main.py, representing courses, lectures, materials, and other relevant entities.

8. **kb_ntriples.rdf**: An alternate representation of the knowledge base in the N-Triples format. It provides the same information as kb.ttl but in a different serialization format.

9. **Report.pdf**: The project report detailing the design, implementation, and evaluation of the knowledge base construction process.


## Execution Steps

To build the knowledge base, follow these steps:

1. Clone or download this repository.
2. Ensure Python is installed on your system.
4. Open a terminal or command prompt and Navigate to the project directory.
5. Run `pip install -r requirements.txt` to install the required libraries.
5. Run `python mergeCSV.py` to clean and merge the CSV files, ensuring the dataset is up to date and properly formatted.
6. Execute `python main.py` to generate the knowledge base. This process will produce two output files: kb.ttl and kb_ntriples.rdf.
7. Download the Apache Jena Fuseki binary distribution from [here](https://jena.apache.org/download/index.cgi).
8. Unpack the archive and make the server script executable by running `chmod u+x fuseki-server` in the terminal.
9. Access Fuseki's web interface by navigating to http://localhost:3030/ in your browser.
10. Create a new dataset, upload kb.ttl to populate it with data, and proceed to the query tab to start querying data.

## Contributors

- Nilesh Suryawanshi
- Vidhi Sagathiya

---

We hope this README provides clear guidance on navigating our project. Should you encounter any issues or have questions, please feel free to reach out to our team.

Thank you for exploring Roboprof!

