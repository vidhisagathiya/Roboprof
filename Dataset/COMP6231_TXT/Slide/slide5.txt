Intro to AI1Artificial Intelligence: ML: Decision Trees & k-means Clustering   Russell & Norvig: Sections 18.3, 18.42Today1. Introduction to ML (contd.)2. Decision Trees3. Evaluation (contd.)4. Unsupervised Learning: k-means Clustering https://medium.com/machine-learning-for-humans/neural-networks-deep-learning-cdad8aeae49b4What is Machine Learning? Learning = crucial characteristic of an intelligent agent ML Constructs algorithms that learn from data i.e., perform tasks that were not explicitly programmed and improve their performance the more tasks they accomplish generalize from given experiences and are able to make judgments in new situations Applications Too many to list here! Recommender systems (eg. Netflix)  Pattern Recognition (eg. Handwriting recognition) Detecting credit card fraud Computer vision (eg. Object recognition) Discovering Genetic Causes of Diseases  Natural Language Processing (eg. Spam filtering) Speech Recognition / Synthesis Medical Diagnostics Information Retrieval (eg. Image search) Learning heuristics for game playing … Oh… I’m out of space Types of Machine Learning6http://www.cognub.com/index.php/cognitive-platform/7Types of Learning In Supervised learning We are given a training set of (X, f(X)) pairs  In Reinforcement learning We are not given the (X, f(X)) pairs But we get a reward when our learned f(X) is right, and we try to maximize the reward Goal: maximize the nb of right answers   In Unsupervised learning We are only given the Xs - not the corresponding f(X) No teacher involved / Goal: find regularities among the Xs (clustering) Data miningbig nose big teeth big eyes no moustache f(X) = not personsmall nose small teeth small eyes no moustache f(X) = personsmall nose big teeth small eyes moustache f(X) = ?small nose big teeth small eyes moustache f(X) = ?big nose big teeth big eyes no moustache not givensmall nose small teeth small eyes no moustache not givensmall nose big teeth small eyes moustache f(X) = ?8Logical Inference Inference: process of deriving new facts from a set of premises Types of logical inference:1. Deduction2. Abduction 3. Induction9Deduction aka Natural Deduction Conclusion follows necessary from the premises.  From A  B and ⇒ A, we conclude that B We conclude from the general case to a specific example of the general case  Ex:All men are mortal.Socrates is a man.Socrates is mortal.10Abduction Conclusion is one hypothetical (most probable) explanation for the premises From A ⇒ B and B, we conclude A Ex:Drunk people do not walk straight.John does not walk straight.John is drunk. Not sound… but may be most likely explanation for B Used in medicine… in reality… disease ⇒ symptoms patient complains about some symptoms… doctor concludes a disease 11Induction Conclusion about all members of a class from the examination of only a few member of the class.   From A C∧   ⇒ B and A D ∧  ⇒ B, we conclude A⇒B We construct a general explanation based on a specific case.  Ex:All CS students in COMP 6721 are smart.All CS students on vacation are smart.All CS students are smart. Not sound But, can be seen as  hypothesis construction or generalisation12Inductive Learning = learning from examples  Most work in ML  Examples are given (positive and/or negative) to train a system in a classification (or regression) task Extrapolate from the training set to make accurate predictions about future examples Can be seen as learning a function Given a new instance X you have never seen You must find an estimate of the function f(X)  where f(X) is the desired output Ex:  X = features of a face (ex. small nose, big teeth, …) f(X) = function to tell if X represents a human face or not small nose big teeth small eyes moustache f(X) = ?XTypes of Machine Learning13http://www.cognub.com/index.php/cognitive-platform/14Types of Machine Learning Supervised learning We are given a training set of (X, f(X)) pairs X = <color, length>   Unsupervised learning We are only given the Xs - not the corresponding f(X)??sea bass salmon15Example Given  pairs (X,f(X)) (the training set – the data points) Find a function that fits the training set well So that given a new X, you can predict its f(X) value Note: choosing one function over another beyond just looking at the training set is called inductive bias (eg. prefer "smoother" functions)basssalmonlengthcolorPossible decision boundarydecision regionsOther possible decision boundary16Inductive Learning Framework Input data are represented by a vector of features, X Each vector X is a list of (attribute, value) pairs. Ex: X = [nose:big, teeth:big, eyes:big, moustache:no]  The number of attributes is fixed (positive, finite) Each attribute has a fixed, finite number of possible values  Each example can be interpreted as a point in a n-dimensional feature space where n is the number of attributesNote: attribute == feature17ExampleReal ML applications typically require hundreds, thousands or millions of examplessource: Alison Cawsey: The Essence of AI (1997).18Techniques in ML Probabilistic Methods ex: Naïve Bayes Classifier Decision Trees Use only discriminating features as questions in a big if-then-else tree Neural networks  Also called parallel distributed processing or connectionist systems Intelligence arise from having a large number of simple computational units …19Today1. Introduction to ML (contd.)2. Decision Trees3. Evaluation (contd.)4. Unsupervised Learning: k-means Clustering https://medium.com/machine-learning-for-humans/neural-networks-deep-learning-cdad8aeae49bGuess Who?2021Decision Trees Simple, but very successful form of learning algorithm Very well-know algorithm is ID3 (Quinlan, 1987) and its successor C4.5 Look for features that are very good indicators of the result, place these features (as questions) in nodes of the tree Split the examples so that those with different values for the chosen feature are in a different set  Repeat the same process with another featureID3 / C4.5 Algorithm Once the feature is selected for the current node, generate children nodes, one for each possible value of the selected attribute Partition the examples using the possible values of this attribute, and assign these subsets of the examples to the appropriate child node Repeat for each child node until all examples associated with a node are classifiedhttp://www.rulequest.com/Personal/ Top-down construction of the decision tree  Recursive selection of the “best feature” to use at the current node in the tree23ExampleInfo on last year’s students to determine if a student will get an ‘A’ this yearFeatures (X) Output f(X)Student ‘A’ last year? Black hair? Works hard? Drinks? ‘A’ this year?X1: Richard Yes Yes No Yes No X2: Alan Yes Yes Yes No Yes X3: Alison No No Yes No No  X4: Jeff No Yes No Yes NoX5: Gail Yes No Yes Yes Yes X6: Simon No Yes Yes Yes No → Worksheet #4 (Decision Tree)25Example 2: The Restaurant Goal: learn whether one should wait for a table Attributes Alternate: another suitable restaurant nearby Bar: comfortable bar for waiting Fri/Sat: true on Fridays and Saturdays Hungry: whether one is hungry Patrons: how many people are present (none, some, full) Price: price range ($, $$, $$$) Raining: raining outside Reservation: reservation made Type: kind of restaurant (French, Italian, Thai, Burger) WaitEstimate: estimated wait by host (0-10 mins, 10-30, 30-60, >60)26Example 2: The Restaurant Training data:source: Norvig (2003)27A First Decision Tree But is it the best decision tree we can build?source: Norvig (2003)28Ockham’s Razor Principle   “It is vain to do more than can be done with less… Entities  should not be multiplied beyond necessity.“[Ockham, 1324] In other words… always favor the simplest answer that correctly fits the training data i.e. the smallest tree on average This type of assumption is called inductive bias inductive bias = making a choice beyond what the training instances contain29Finding the Best Tree can be seen as searching the space of all possible decision trees Inductive bias: prefer shorter trees on average how? search the space of all decision trees  always pick the next attribute to split the data based on its "discriminating power" (information gain) in effect, steepest ascent hill-climbing search where heuristic is information gainsource: Tom Mitchell, Machine Learning (1997)empty treecomplete tree30Which Tree is Best? F1?F2? F3?F4? F5? F6? F7?class class class class class class class classF1?F2?F3?classclassclass F4?class F5?class F6?class F7?class class31Smaller trees are betterWhat’s the size of a tree? Number of leaves Height of the tree Longest path in the tree from the root to a leaf External Path Length Start at leaf, go up to the root and count the number of edges  Do this for every leaf and add up the numbers Weighted External Path Length Idea: not all paths are equally important/likely Use the training data to computed a weighted sum32Choosing the Next AttributeThe key problem is choosing which feature to split a given set of examplesID3 uses Maximum Information-Gain: Choose the attribute that has the largest information gain i.e., the attribute that will result in the smallest expected size of the subtrees rooted at its children   information theory33Intuitively… Patron: If value is Some… all outputs=Yes If value is None… all outputs=No If value is Full… we need more tests Type: If value is French… we need more tests If value is Italian… we need more tests If value is Thai… we need more tests If value is Burger… we need more tests … So patron  may lead to shorter tree…Output f(X)source: Norvig (2003)34Next Feature… For only data where patron = Full hungry: If value is Yes… we need more tests If value is No… all output = No type: If value is French… all output = No If value is Italian… all output = No If value is Thai… we need more tests If value is Burger… we need more tests … So hungry is more discriminating (only 1 new branch)…35A Better Decision Tree 4 tests instead of 9  11 branches instead of 21 source: Norvig (2003)36Essential Information Theory Developed by Shannon in the 1940s Notion of entropy (information content) Measure how “predictable” a RV is...   If you already have a good idea about the answer (e.g. 90/10 split)→ low entropy If you have no idea about the answer (e.g. 50/50 split)→ high entropy37Choosing the Next Attribute The key problem is choosing which feature to split a given set of examples Most used strategy: information theory Entropy (or information content)entropy of a fair coin toss (the RV) with 2 possible outcomes, each with a probability of 1/2 )p(x)logp(xH(X) iXx2iibit 121log2121log21  21,21H  )p(x)logp(xtoss) coin H(fair22iXx2ii  -38Why -p(x)·log2(p(x))39Entropy Let X be a discrete random variable (RV) with n possible outcomes x1 … xn Entropy (or information content) measures the amount of information in a RV average uncertainty of a RV the average length of the message needed to transmit an outcome xi of that variable  measured in bits for only 2 outcomes x1 and x2, then 1 ≥ H(X) ≥ 0 n1ii2i )p(x)logp(xH(X)→ Worksheet #4 (Information Content)40Example: The Coin Flip Fair coin: Rigged coin:P(head)Entropyfair coin -> high entropyrigged coin -> low entropybit 121log2121log21 - )p(x)logp(xH(X) 22n1ii2i   bits 0.081001log100110099log10099 - )p(x)logp(xH(X) 22n1ii2i   → Worksheet #4 (Entropy)41Choosing the Best Feature (con't) The "discriminating power" of an attribute A given a data set S Let Values(A)  = the set of values that attribute A can take Let Sv = the set of examples in the data set which have value v for attribute A (for each value v from Values(A) )information gain (or entropy reduction) vvalues(A)  vv SH xSSH(S)                 A)|H(SH(S)A) gain(S,42Some IntuitionSize Color Shape OutputBig Red Circle +Small Red Circle +Small Red Square -Big Blue Circle - Size is the least discriminating attribute (i.e. smallest information gain) Shape and color are the most discriminating attributes (i.e. highest information gain)43A Small Example (1)n Color   n Red: 2+ 1-  n Blue: 0+ 1- Size Color Shape OutputBig Red Circle +Small Red Circle +Small Red Square -Big Blue Circle -Values(Color) = {red,blue}142log4242log42H(S) 22   vor)values(Col  vv SH xSSH(S) Color) gain(S,  0.31150.6885-1  Color)|H(S - H(S)  )gain(Color       0.6885  (0)41(0.918)43Color)|H(S011log111,0Hblue)  Color |H(S       0.91831log3132log3231,32Hred)  Color |H(S       or)Values(Col of v each for222 44A Small Example (2) Note: by definition, Log 0 = -∞  0log0 is 0n Shape  n Circle: 2+ 1-   n Square: 0+ 1-   Size Color Shape OutputBig Red Circle +Small Red Circle +Small Red Square -Big Blue Circle -0.31150.6885-1  Shape)|H(S - H(S)  )gain(Shape0.6885(0)41(0.918)43Shape)|H(S142log4242log42H(S) 22  46A Small Example (3)n Size  n Big: 1+ 1- n Small: 1+ 1-   Size Color Shape OutputBig Red Circle +Small Red Circle +Small Red Square -Big Blue Circle -142log4242log42H(S) 22  → Worksheet #4 (“Information Gain”)47A Small Example (4) So first separate according to either color or shape (root of the tree)Size Color Shape OutputBig Red Circle +Small Red Circle +Small Red Square -Big Blue Circle -0  gain(Size)0.3115  )gain(Color0.3115  )gain(Shape48A Small Example (5) Let’s assume we pick Color for the root…Size Color Shape OutputBig Red Circle +Small Red Circle +Small Red Square -Big Blue Circle -ColorSize? or Shape? -blueredS2 31log3132log32)H(S 2220Size50Back to the Restaurant Training data:source: Norvig (2003)51 Attribute pat (Patron) has the highest gain, so root of the tree should be attribute Patrons do recursively for subtreesThe Restaurant Example0.541bits...44log44   40log 40x12422log 2220log 20- x 122164,62H x 12644,40H x 12422,20H x 1221gain(pat)2222   bits 042,42H x12442,42H x 12421,21H x 12221,21H x 1221gain(type) ...gain(alt) ...gain(bar) ...gain(fri) ...gain(hun)...)gain(price ...gain(rain) ...gain(res)...gain(est)Decision Boundaries of Decision TreesFeature 2Feature 152Decision Boundaries of Decision Treest1 Feature 2Feature 1Feature 2 > t1??53Decision Boundaries of Decision Treest1t2Feature 2Feature 1Feature 2 > t1Feature 1 > t2??54Decision Boundaries of Decision Treest1t3t2Feature 2Feature 1Feature 2 > t1Feature 1 > t2Feature 2 > t35556Applications of Decision Trees One of the most widely used learning methods in practice  Fast, simple, and traceable (explainable AI!)  Can out-perform human experts in many problems57Today1. Introduction to ML (contd.)2. Decision Trees3. Evaluation (contd.)4. Unsupervised Learning: k-means Clustering https://medium.com/machine-learning-for-humans/neural-networks-deep-learning-cdad8aeae49b60Metrics, revisited Accuracy  % of instances of the test set the algorithm correctly classifies when all classes are equally important and represented  Recall, Precision & F-measure when one class is more important and the others62Confusion MatrixTPTP+FNRecall=TP+FPTPPrecision = Not all errors are equal Type I error (FP) might be worse than Type II error (FN)(depends on the application, e.g., spam filtering) “It is better to risk saving a guilty man than to condemn an innocent one.” (Voltaire)TP+TNTP+TN+FP+FNAccuracy=  TPTP+FNRecall=TPTP+FNRecall=   Model says… In reality, the instance is… in class C Is not in class C    instance is in class C True Positive (TP) False Positive  (FP)    instance is NOT in class C False Negative (FN) True Negative (TN)   64Evaluation: A Single Value Measure cannot take mean of P&R if R = 50%    P = 50% M = 50% if R = 100%  P = 10% M = 55% (not fair) take harmonic meanHM is high only when both P&R are highif R = 50% and P = 50%    HM = 50%if R = 100% and P = 10%   HM = 18.2% take weighted harmonic meanwr: weight of R wp: weight of P a = 1/wr b= 1/wp let β2 = a/b                                                                … which is called the F-measureP1R12HM     P1bRabaPbbRbabb)(aPbRabaWHM1RPβPR 1)(βP1Rβ1βWHM 2222 65Evaluation: the F-measure A weighted combination of precision and recall β represents the relative importance of precision and recall when β = 1, precision & recall have same importance when β > 1, precision is favored when β < 1, recall is favoredR)P(β1)PR(βF 2267Example✔ ❌ ✔ ✔✔ ❌ ❌ ✔✔ ❌ ✔ ✔✔ ❌ ✔ ✔✔ ❌ ❌ ✔❌ ❌ ❌ ✔❌ ❌ ❌ ✔❌ ❌ ❌❌ ❌ ❌❌ ❌ ❌ ❌(B=1)     → Worksheet #4 (F-Measure)68Error Analysis Where did the learner go wrong ? Use a confusion matrix / contingency table correct class (that should have been assigned) classes assigned by the learner  C1 C2 C3 C4 C5 C6 … Total C1 94 3 0 0 3 0  100 C2 0 93 3 4 0 0  100 C3 0 1 94 2 1 2  100 C4 0 1 3 94 2 0  100 C5 0 0 3 2 92 3  100 C6 0 0 5 0 10 85  100 …           69A Learning Curve  Size of training set  the more, the better but after a while, not much improvement…source: Mitchell (1997)70Some Words on Training In all types of learning… watch out for: Noisy input Overfitting/underfitting the training data71Noisy Input In all types of learning… watch out for: Noisy Input: Two examples have the same feature-value pairs, but different outputs  Some values of features are incorrect or missing (ex. errors in the data acquisition) Some relevant attributes are not taken into account in the data set Size Color Shape OutputBig Red Circle +Big Red Circle -Overfitting Complicated boundaries overfit the data they are too tuned to the particular training data at hand They do not generalize well to the new data  Extreme case: “rote learning” Training error is low Testing error is high  If a large number of irrelevant features are there, we may find meaningless regularities in the data that are particular to the training data but irrelevant to the problem.Underfitting We can also underfit data, i.e. use too simple decision boundary  Model is not expressive enough (not enough features) There is no way to fit a linear decision boundary so that the training examples are well separated Training error is high Testing error is high 74Cross-validation K-fold cross-validation  run k experiments, each time you test on 1/k of the data, and train on the rest then you average the results ex: 10-fold cross validation1. Collect a large set of examples (all with correct classifications)2. Divide collection into two disjoint sets:  training (90%) and test (10% = 1/k)3. Apply learning algorithm to training set 4. Measure performance with the test set5. Repeat steps 2-4, with the 10 different portions6. Average the results of the 10 experimentsexp1: train testexp2: train test trainexp3: train test train… …75Today1. Introduction to ML (contd.)2. Decision Trees3. Evaluation (contd.)4. Unsupervised Learning: k-means Clustering https://medium.com/machine-learning-for-humans/neural-networks-deep-learning-cdad8aeae49bTypes of Machine Learning76http://www.cognub.com/index.php/cognitive-platform/Remember this slide?77sea bass78Unsupervised Learning Learn without labeled examples  i.e. X is given, but not f(X)  Without a f(X), you can't really identify/label a test instance But you can: Cluster/group the features of the test data into a number of groups  Discriminate between these groups without actually labeling themsmall nose big teeth small eyes moustache f(X) = ?79What is Clustering The organization of unlabeled data into similarity groups called clusters. A cluster is a collection of data items which are “similar” between them, and “dissimilar” to data items in other clusters.80Historic Application of Clustering John Snow, a London physician plotted the location of cholera on a map during an outbreak in the 1850s. The locations indicated that cases were clustered arounds certain intersections where there were polluted wells – thus exposing both the problem and the solution.FROM: Nina Mishra HP Labs81Clustering Represent each instance as a vector <a1, a2, a3,…, an> Each vector can be visually represented in an n-dimensional spaceX2X1 X3a1 a2 a3 OutputX1 1 0 0 ?X2 1 6 0 ?X3 8 0 1 ?X4 6 1 0 ?X5 1 7 1 ?X4X582Clustering Clustering algorithm Represent test instances on a n dimensional space  Partition them into regions of high density How?  … many algorithms (ex. k-means) Compute the centroïd of each region as the  average of data points in the cluster83Clustering TechniquesK-means84k-means Clustering User selects how many clusters they want… (the value of k)1. Place k points into the space (ex. at random). These points represent initial group centroïds.2. Assign each data point xn to the nearest centroïd.3. When all data points have been assigned, recalculate the positions of the K centroïds as the average of the cluster4. Repeat Steps 2 and 3 until none of the data instances change group.85Euclidean Distance101 2 3 4 5 6 7 8 9 10123456789 To find the nearest centroïd… a possible metric is the Euclidean distance distance between 2 pts p = (p1, p2, ....,pn)q = (q1, q2, ....,qn)   where to assign a data point x? For all k clusters, chose the one where x has the smallest distance n1i2ii qpd860123450 1 2 3 4 5Example (in 2-D… i.e. 2 features)initial 3 centroïds (ex. at random)c1c2c3870123450 1 2 3 4 5Examplepartition data points to closest centroïdc1c2c3880123450 1 2 3 4 5Examplere-compute new centroïdsc1c2c3890123450 1 2 3 4 5Examplere-assign data points to new closest centroïdsc1c2c390Examplec1c2 c30123450 1 2 3 4 5→ Worksheet #4 (k-Means Clustering)92Why use k-means? Strengths: Simple Easy to understand and implement Efficient: Time complexity O(t·k·n) n number of data points k number of clusters t number of iterations With small k and t, linear performance on practical problems93Weakness of k-means User needs to specify k Algorithm is sensitive to outliers i.e., data points that are far away from others Could be errors in the data or special data points with very different characteristics94Outliersoutlier(A) Undesirable clusters(B) Ideal clustersoutlier95Special data structures(A) Two natural clusters                                      (B) k-means clusters96Sensitivity to initial seeds97K-means: Summary Despite weaknesses, k-means is still one of the most popular algorithms, due to its simplicity and efficiency No clear evidence that any other clustering algorithm performs better in general Comparing different clustering algorithms is a difficult task. No one knows the correct clusters!98Today1. Introduction to ML (contd.)2. Decision Trees3. Evaluation (contd.)4. Unsupervised Learning: k-means Clustering https://medium.com/machine-learning-for-humans/neural-networks-deep-learning-cdad8aeae49b