COMP 6721: State Space Search1Artificial Intelligence: State Space Search   Many slides from: robotics.stanford.edu/~latombe/cs121/2003/home.htmhttp://robotics.stanford.edu/~latombe/cs121/winter02/home.htm2Motivation2Rubik’s cube TetrisGoogle itinerary8-puzzle3Today State Space Representation State Space Search Uninformed search Breadth-first and Depth-first Depth-limited Search  Iterative Deepening Uniform Cost Informed search  Hill climbing Best-First (Designing Heuristics) A* SummarySearchKnowledgerep.PlanningReasoningLearningAgentRoboticsPerceptionNaturallanguage... ExpertSystemsConstraint  satisfaction   4Example: 8-Puzzle123 45 678 1 2 34 5 67 8Initial state Goal stateState: Any arrangement of 8 numbered tiles and an empty tile on a 3x3 boardthere are several standard goals states for the 8-puzzle1 2 34 67 81 2 3478655…5(n2-1)-puzzle123 45 67812151114101395 6 7 84321....8-puzzle15-puzzle615-PuzzleInvented in 1874 by Noyes Palmer Chapman  … but Sam Loyd claimed he invented it!715-PuzzleSam Loyd even offered $1,000 of his own money to the first person who would solve the following problem:12141115101395 6 7 8432112151114101395 6 7 84321?8But no one ever won the prize…9State Space Many AI problems, can be expressed in terms of going from an initial state to a goal state Ex: to solve a puzzle, to drive from home to Concordia… Often, there is no direct way to find a solution to a problem but we can list the possibilities and search through them1. Brute force search:  generate and search all possibilities (but inefficient)2. Heuristic search:  only try the possibilities that you think (based on your current best guess) are more likely to lead to good solutions10State Space Problem is represented by:1. Initial State starting state  ex. unsolved puzzle, being at home2. Set of operators actions responsible for transition between states3. Goal test function Applied to a state to determine if it is a goal state ex. solved puzzle, arrived at Concordia4. Path cost function Assigns a cost to a path to tell if a path is preferable to another Search space: the set of all states that can be reached from the initial state by any sequence of action Search algorithm:  how the search space is visited11Example: The 8-puzzleSet of operators: blank moves up, blank moves down, blank moves left, blank moves right Goal test function:state matches the goal statePath cost function:each movement costs 1so the path cost is the length of the path (the number of moves)source: G. Luger (2005) 123 45 678 1 2 34 5 67 8Initial state Goal state128-Puzzle: Successor Function123 45 678123 45678123 45 678123 45 678Search is about the exploration of alternatives13State Graph Each state is represented by a distinct node  An arc (or edge) connects a node s to a node s’ if s’  ∈ SUCCESSOR(s)  The state graph may contain more than one connected component15Just to make sure we’re clear…14752638Initial state64715283Goal state16State Space as a Search TreeSearch tree In graph representation, cycles can prevent termination Blind search without cycle check may never terminate Use a tree representation, and check for cycles17State Space for the 8-puzzlesource: G. Luger (2005) DownLeftUpRight18How large is the state space of the (n2-1)-puzzle? Number of states: 8-puzzle  -->  9! = 362,880 states 15-puzzle  -->  16! ~ 2.09 x 1013 states 24-puzzle  -->  25! ~ 1025 states At 100 millions states/sec: 8-puzzle --> 0.036 sec 15-puzzle --> ~ 55 hours 24-puzzle  -->  > 109 years19Today State Space Representation State Space Search Uninformed search Breadth-first and Depth-first Depth-limited Search  Iterative Deepening Uniform Cost Informed search  Hill climbing Best-First (Designing Heuristics) A* Summary20Uninformed VS Informed Search  Uninformed search  We systematically explore the alternatives aka: systematic/exhaustive/blind/brute force search Breadth-first Depth-first Uniform-cost Depth-limited search Iterative deepening search Bidirectional search  … Informed search (heuristic search) We try to choose smartly Hill climbing Best-First A* …21Today State Space Representation State Space Search Uninformed search Breadth-first and Depth-first Depth-limited Search  Iterative Deepening Uniform Cost Informed search  Hill climbing Best-First (Designing Heuristics) A* Summary22Breadth-first vs Depth-first Search Determine order for examining states Depth-first: visit successors before siblings Breadth-first: visit siblings before successors  i.e. visit level-by-levelsource: G. Luger (2005) 23Data Structures In all search strategies, you need:  open list (aka the frontier) lists generated nodes not yet expanded order of nodes controls order of search  closed list (aka the explored set) stores all the nodes that have already been visited (to avoid cycles). ex:Closed = [A, B, C, D, E]Open = [F, G, H, I, J, K, L]source: G. Luger (2005) 24Data StructuresDepth = length of path from root to node PARENT-NODE123 45 678 STATE REPRESENTATIONBOOKKEEPING5Path-Cost5DepthRightAction...CHILDREN To trace back the entire path of the solution after the search, each node in the lists contain: 25Generic Search Algorithm 1. Initialize the open list  with the initial node so (top node) 2. Initialize the closed list   to  empty3. Repeata) If the open list  is empty, then exit with failure.b) Else, take the first node s  from the open list. c) If s is a goal state, exit  with success.  Extract the solution path from s to sod) Else, insert s in the closed list (s has been visited /expanded) e) Insert the successors of s  in  the  open list  in a certain order if they are not already in the closed and/or open lists (to avoid cycles) Note:   The order of the nodes in the open list depends on the search strategy26 DFS and BFS differ only in the way they order nodes in the open list:  DFS uses a stack:     nodes are added on the top of the list.  BFS uses a queue:   nodes are added at the end of the list.DFS and BFS27Breadth-First Searchsource: G. Luger (2005) 28Breadth-First Search Example BFS: (open is a queue)Assume U is goal statesource: G. Luger (2005) 1. open = [A-null]  closed = []2. open = [B-A  C-A  D-A]  closed [A]3. open = [C-A D-A  E-B F-B]  closed = [B A]4. → Worksheet #1 (“Breadth-First Search”)...and so on until either U is found or open = []30Snapshot of BFS Search graph at iteration 6 of breadth-first search States on open and closed are highlightedsource: G. Luger (2005) 31Function Depth-First Searchsource: G. Luger (2005) 1. open = [A-null]  closed = []2. open = [B-A  C-A  D-A]  closed [A]3. open = [E-B F-B C-A D-A]  closed = [B A]4. open = [K-E L-E F-B C-A D-A]  closed = [E B A]5. open = [S-K L-E F-B C-A D-A]  closed = [K E B A]→ Worksheet #1 (“Depth-First Search”)32Depth-First Search Examplesource: G. Luger (2005)  DFS: (open is a stack)Assume U is goal state34Snapshot of DFS Search graph at iteration 6 of depth-first search States on open and closed are highlightedsource: G. Luger (2005) 35 Breadth-first: Optimal: will always finds shortest pathBut: inefficient if branching factor B is very high memory requirements high -- exponential space for states required: Bn Depth-first: Not optimal (no guarantee to find the shortest path)But: Requires less memory But both search are impractical in real applications because search space is too large!Depth-first vs. Breadth-first36Today State Space Representation State Space Search Uninformed search Breadth-first and Depth-first Depth-limited Search  Iterative Deepening Uniform Cost Informed search  Hill climbing Best-First (Designing Heuristics) A* Summary37Depth-Limited SearchCompromise for DFS : Do depth-first but with depth cutoff k (depth at which nodes are not expanded) Three possible outcomes: Solution Failure (no solution) Cutoff (no solution within cutoff)38Today State Space Representation State Space Search Uninformed search Breath-first and Depth-first Depth-limited Search  Iterative Deepening Uniform Cost Informed search  Hill climbing Best-First (Designing Heuristics) A* Summary39Compromise between BFS and DFS: use depth-first search, but with a maximum depth before going to next level Repeats depth first search with gradually increasing depth limits Requires little memory (fundamentally, it’s a depth first) Finds the shortest path (limited depth) Preferred search method when there is a large search space and the depth of the solution is unknown Iterative Deepening40Iterative Deepening: Examplesource: Russel & Norvig (2003) 41Iterative Deepening: Examplesource: Russel & Norvig (2003) 42Iterative Deepening: Examplesource: Russel & Norvig (2003) 43Iterative Deepening: Examplesource: Russel & Norvig (2003) 44Today State Space Representation State Space Search Uninformed search Breath-first and Depth-first Depth-limited Search  Iterative Deepening Uniform Cost Informed search  Hill climbing Best-First (Designing Heuristics) A* Summary45 Breadth First Search  Open is a priority queue sorted using the depth of the nodes from the root guarantees to find the shortest solution path But what if all edges/moves do not have the same cost?Uniform Cost Search Uniform Cost Search uses a priority queue sorted using the cost from the root to node n – later called g(n) guarantees to find the lowest cost solution path4263346Today State Space Representation State Space Search Uninformed search Breath-first and Depth-first Depth-limited Search  Iterative Deepening Uniform Cost Informed search  Hill climbing Best-First (Designing Heuristics) A* Summary47 Most of the time, it is not feasible to do an exhaustive search, search space is too large e.g. state space of all possible moves in chess = 10120  1075 = nb of molecules in the universe 1026 = nb of nanoseconds since the “big bang” so far, all search algorithms have been uninformed (general search) so need an informed/heuristic search  Idea:  choose "best" next node to expand according to a selection function (i.e. a heuristic function h(n)) But: heuristic might fail Informed Search (aka heuristic search)48Heuristic - Heureka!  Heuristic:  a rule of thumb, a good bet but has no guarantee to be correct whatsoever! Heuristic search: A technique that improves the efficiency of search, possibly sacrificing on completeness Focus on paths that seem most promising according to some function Need an evaluation function (heuristic function) to score a node in the search tree Heuristic function h(n) = an approximation of the lowest cost from node n to the goalg(n)4942633 g(n) = cost of current path from start to node n2 1h(n)50263 h(n) = estimate of the lowest cost from n to goal51Today State Space Representation State Space Search Uninformed search Breath-first and Depth-first Depth-limited Search  Iterative Deepening Informed search  Hill Climbing Best-First (Designing Heuristics) A* Summary52Example: Hill Climbing with Blocks World Heuristic: 0pt if a block is sitting where it is supposed to sit +1pt if a block is NOT sitting where it is supposed to sit so lower h(n) is better h(initial) = 2 h(goal) = 0 Operators: pickup&putOnTable(Block) pickup&stack(Block1,Block2)source: Rich & Knight, Artificial Intelligence, McGraw-Hill College 1991.  53pickup&stack(H,A)pickup&putOnTable(A)pickup&stack(A,H) pickup&putOnTable(H)h(n) = 2h(n) = 2h(n) = 2h(n) = 1h(n) = 2Example: Hill Climbing with Blocks World54Hill Climbing General hill climbing strategy:  as soon as you find a position that is better than the current one, select it.  Does not maintain a list of next nodes to visit (an open list) Similar to climbing a mountain in the fog with amnesia … always go higher than where you are now,        but never go back… Steepest ascent hill climbing: instead of moving to the first position that      is better than the current one pick the best position out of all the next possible moves help!55Steepest Ascent Hill ClimbingcurrentNode = startNode;     loop do         L = CHILDREN(currentNode);         nextEval = INFINITY;         nextNode = NULL;         for all c in L             if (HEURISTIC-VALUE(c) < nextEval) // lower h is better                 nextNode = c;                 nextEval = HEURISTIC-VALUE(c);         if nextEval >= HEURISTIC-VALUE(currentNode)            // Return current node since no better child state exists            return currentNode;         currentNode = nextNode;Adapted from https://en.wikipedia.org/wiki/Hill_climbing56Example: Hill Climbing with Blocks Worldpickup&stack(H,A)pickup&putOnTable(A)pickup&stack(A,H) pickup&putOnTable(H)h(n) = 2h(n) = 2h(n) = 2h(n) = 1h(n) = 2hill-climbing will stop,because all children havehigher h(n) than theparent… --> local minimumDon’t be confused… a lower h(n) is better…57Problems with Hill Climbing  Foothills (or local maxima)  reached a local maximum, not the global maximum a state that is better than all its neighbors but is not better than some other states farther away.  at a local maximum, all moves appear to make things worse.   ex: 8-puzzle: we may need to move tiles temporarily out of goal position in order to place another tile in goal position ex: TSP: "nearest neighbour" heuristich(n) Some parameter to represent nSome other parameter to represent n58Problems with Hill Climbing  Plateau a flat area of the search space in which the next states have the same value.  it is not possible to determine the best direction in which to move by making local comparisons. source: Rich & Knight, Artificial Intelligence, McGraw-Hill College 1991. 59Some Solutions to Hill-Climbing   Random-restart hill-climbing  random initial states are generated run each until it halts or makes no significant progress.  the best result is then chosen. keep going even if the best successor has the same value as current node works well on a "shoulder" but could lead to infinite loop     on a plateausource: Rich & Knight, Artificial Intelligence, McGraw-Hill College 1991. & Russel & Norvig (2003) 60Today State Space Representation State Space Search Uninformed search Breadth-first and Depth-first Depth-limited Search  Iterative Deepening Informed search  Hill Climbing Best-First (Designing Heuristics) A* Summary61Best-First Search problem with hill-climbing: one move is selected and all others are forgotten. solution to hill-climbing:  use "open" as a priority queue this is called best-first search Best-first search: Insert nodes in open list so that the nodes are sorted in ascending  h(n) Always choose the next node to visit to be the one with the best h(n) -- regardless of where it is in the search space62Best-First: ExampleLower h(n) is bettersource: Rich & Knight, Artificial Intelligence, McGraw-Hill College 1991.  63Notes on Best-first If you have a good h(n), best-first can find the solution very quickly The first solution may not be the best, but there is a good chance of finding it quickly It is an exhaustive search … will eventually try all possible paths64Best-First Search: ExampleLower h(n) is bettersource: adapted from G. Luger (2005) P-01. open = [A-null-5]  closed = []2. open = [B-A-4  C-A-4  D-A-6]  (arbitrary choice) closed [A]3. open = [C-A-4 E-B-5 F-B-5  D-A-6]  closed = [B A]4. → Worksheet #1 (“Best-First Search”)66Today State Space Representation State Space Search Uninformed search Breadth-first and Depth-first Depth-limited Search  Iterative Deepening Informed search  Hill Climbing Best-First (Designing Heuristics) A* Summary67Designing Heuristics Heuristic evaluation functions are highlydependent on the search domain In general: the more informed a heuristic is,the better the search performance Bad heuristics lead to frequent backtracking So how do we design a “good” heuristic?68Example: 8-Puzzle – Heuristic 1 h1: Simplest heuristic Hamming distance : count number of tiles out of place when compared with goal h1(n)  = 6 does not consider the distance tiles have to be movedsource: G. Luger (2005) 14752638STATE n64715283Goal state69Example: 8-Puzzle – Heuristic 2 h2: Better heuristic Manhattan distance: sum up all the distances by which tiles are out of place h2(n) = 2+3+0+1+3+0+3+1              = 13source: G. Luger (2005) 14752638STATE n64715283Goal state70Example: 8-Puzzle – Heuristic 3 h3: Even Better sum of permutation inversions See next slide…source: G. Luger (2005) 71 For each numbered tile, count how many tiles on its right should be on its left in the goal state.   h3(n) = n5 + n8 + n4 + n2 + n1 + n7 + n3 + n6   = 4  + 6  + 3   + 1   + 0  + 2   + 0  + 0    = 16 h3(N) = sum of permutation inversions14752638STATE n5 8 4 2 1 7 3 664715283Goal state72 h1(n)  = misplaced numbered tiles               = 6 h2(n)  =  Manhattan distance           = 2 + 3 + 0 + 1 + 3 + 0 + 3 + 1 = 13 h3(n) = sum of permutation inversions        = n5 + n8 + n4  + n2 + n1 + n7 + n3 + n6  = 4  + 6  + 3   + 1   + 0  + 2  + 0  + 0  = 16Heuristics for the 8-Puzzle14752638STATE n64715283Goal state73g(n), h(n) and f(n) Evaluation function f(n) = g(n) + h(n) for node n: g(n) current cost from start to node n h(n) estimate of the lowest cost from n to goal f(n) estimate of the lowest cost of the solution path (from start to goal passing through n) Now consider f*(n) = g*(n) + h*(n): g*(n) cost of lowest cost path from start to node n h*(n) actual lowest cost from n to goal f*(n) actual cost of lowest cost of the solution path (from start to goal passing through n)74Evaluating Heuristics1. Admissibility: “optimistic” never overestimates the actual cost of reaching the goal guarantees to find the lowest cost solution path to the goal (if it exists)2. Monotonicity: “local admissibility” guarantees to find the lowest cost path to each state n encountered in the search3. Informedness: measure for the “quality” of a heuristic the more informed, the better75Admissibility A heuristic is admissible if it never overestimates the cost of reaching the goal i.e.: h(n) ≤ h*(n)  for all n guarantees to find the lowest cost solution path to the goal (if it exists) Note:  does not guarantee to find the lowest cost search path.  e.g.: breadth-first is admissible  -- it uses f(n) = g(n) + 0 76Example: 8-Puzzle1 2 34 5 67 812345678n goal  h1(n) = Hamming distance = number of misplaced tiles = 6 --> admissible  h2(n) = Manhattan distance = 13 --> admissible77Monotonicity (aka consistent) An admissible heuristics may temporarily reach non-goal states along a suboptimal path A heuristic is monotonic if it always finds the optimal path to each state the 1st time it is encountered ! guarantees to find the lowest cost path to each state n encountered in the search h is monotonic if for every node n and every successor n’ of n: h(n) ≤ c(n,n') + h(n') i.e. f(n) is non-decreasing along any path78Informedness Intuition: number of misplaced tiles is less informed than Manhattan distance  For two admissible heuristics h1 and h2 if h1(n) ≤ h2(n), for all states n then h2 is more informed than h1 h1(n) ≤ h2(n) ≤ h*(n) More informed heuristics search smaller space to find the solution path However, you need to consider the computational cost of evaluating the heuristic… The time spent computing heuristics must be recovered by a better search79Today State Space Representation State Space Search Uninformed search Breadth-first and Depth-first Depth-limited Search  Iterative Deepening Informed search  Hill Climbing Best-First (Designing Heuristics) A* Summary80Algorithm A Heuristics might be wrong: so search could continue down a wrong path Solution: Maintain depth/cost count, i.e., give preference to shorter/least expensive paths Modified evaluation function f: f(n) = g(n) + h(n)  f(n) estimate of total cost along path through n g(n) actual cost of path from start to node n h(n) estimate of cost to reach goal from node n 81Algorithm A on the 8-puzzlesource: G. Luger (2005) → Worksheet #1 (“Algorithm A”)83Algorithm A on the 8-puzzlesource: G. Luger (2005) 84source: G. Luger (2005) Algorithm A on the 8-puzzleBFS vs. Heuristic search(tiles out of place)source: G. Luger (2005) 86Algorithm A vs Algorithm A* if g(n) ≥ g*(n) for all n best-first search with f(n) = g(n) + h(n) is called “algorithm A” if h(n) ≤ h*(n) for all n i.e. h(n) never overestimates the true cost from n to a goal algorithm A used with such an h(n) is called “algorithm A*” an A* algorithm is admissible  i.e. it guarantees to find the lowest cost solution path from the initial state to the goal87Today State Space Representation State Space Search Uninformed search Breath-first and Depth-first Depth-limited Search  Iterative Deepening Informed search  Hill climbing Best-First (Designing Heuristics) A* Summary88SummarySearch Uses h(n)? Open is a…Breadth-first No QueueDepth-first No StackDepth-limited No StackIterative Deepening No StackUniform Cost No Priority queue sorted by g(n)Hill Climbing Yes noneBest-First Yes Priority queue sorted by h(n)Algorithm A- no constraints on h(n)Yes Priority queue sorted by f(n) f(n) = g(n) + h(n) Algorithm A*- same as A, but h(n) must be admissible- guarantees to find the lowest cost solution pathYes Priority queue sorted by f(n) f(n) = g(n) + h(n) 89Today State Space Representation State Space Search Uninformed search Breadth-first and Depth-first Depth-limited Search  Iterative Deepening Uniform Cost Informed search  Hill climbing Best-First A* Summary