COMP 6721: AIArtificial Intelligence: Deep Learning, CNNsmany slides from:  Y. Bengio, A. Ng and Y. LeCun1Today1. Motivation2. Feature Learning3. Early Training of Deep Neural Networks4. CNNs for Image Processing5. Conclusion2History of AI3Deep Learning in the Academic Press (2012-2015)4Deep Learning in the News (2013)Slide from Yoshua Bengio, 2015 5Deep Learning in the News (2012-2014)Slide from Yoshua Bengio, 2015 6Major Breakthroughs  Speech Recognition & Machine Translation (2010+) Skype Translator Image Recognition & Computer Vision (2012+) Natural Language Processing (2014+)  …Figure from Yoshua Bengio, 2015Google TranslateSkype TranslatorGoogle now7Major Breakthroughs  Speech Recognition & Machine Translation (2010+)  Image Recognition & Computer Vision (2012+) Natural Language Processing (2014+) …Figure from Yoshua Bengio, 20153.567% 3.581%4th yearObject recognition Self driving cars8Major Breakthroughs Image Captioning (deep vision + deep NLP)Question Answering Speech Recognition & Machine Translation (2010+) Image Recognition & Computer Vision (2012+) Natural Language Processing (2014+)9ABImage Captioning: Better than humans?10Today1. Motivation2. Feature Learning3. Early Training of Deep Neural Networks4. CNNs for Image Processing5. Deep Learning for NLP6. Conclusion11A Deep Neural Nethttps://qph.ec.quoracdn.net/main-qimg-970d2b5f57b6b5cd13dc11f5371166b2-chttps://devblogs.nvidia.com/parallelforall/mocha-jl-deep-learning-julia/The Google “Inception” deep neural network architecture for image recognition (27 layers) 12https://qph.ec.quoracdn.net/main-qimg-970d2b5f57b6b5cd13dc11f5371166b2-chttps://qph.ec.quoracdn.net/main-qimg-970d2b5f57b6b5cd13dc11f5371166b2-cMatrix Notation13We now represent inputs x, weights W and the bias weights b as matrices:Input to hidden: 1. Compute net activation neth = x · Wih + bih2.Compute activation function (e.g., sigmod): h = S(neth)Hidden to output: o = S(h · Who + bho) → Worksheet #6 (“Matrix Notation”)https://towardsdatascience.com/neural-networks-i-notation-and-building-blocks-817b1d2ea04bInitial Drawbacks1. Standard backpropagation with sigmoid activation function does not scale well with multiple layers Weight of early layers change too slowly (no learning)2. Overfitting Large network -> lots of parameters -> increased capacity to “learn by heart”3. Multilayered ANNs need lots of labeled data  Most data is not labeled :(14Initial Drawbacks (1)1. Standard gradient-based backpropagation does not scale well with multiple layers…When we multiply the gradients many times (for each layer),  it can lead to …a) Vanishing gradient problem:  gradients shrink exponentially with the number of layers  so weight updates get smaller and smaller and weights of early layers change very slowly and network learns very very slowlyb) Exploding gradient problem: multiplying gradients could also make them grow exponentially.   so weight updates get larger and larger and the weights can become so large as to overflow and result in NaN values15Initial Drawbacks (1)https://medium.com/towards-data-science/activation-functions-and-its-types-which-is-better-a9a5310cc8fhttps://medium.com/towards-data-science/activation-functions-neural-networks-1cbd9f8d91d6To help, we can :a) Use other activation functions…b) Do “gradient clipping” (i.e. set bounds on the gradients)16https://medium.com/towards-data-science/activation-functions-and-its-types-which-is-better-a9a5310cc8fhttps://medium.com/towards-data-science/activation-functions-and-its-types-which-is-better-a9a5310cc8fhttps://medium.com/towards-data-science/activation-functions-neural-networks-1cbd9f8d91d6https://medium.com/towards-data-science/activation-functions-neural-networks-1cbd9f8d91d6Initial Drawbacks (2)https://www.kdnuggets.com/2015/04/preventing-overfitting-neural-networks.html 2. Overfitting Large network -> lots of parameters -> increased capacity to “learn by heart” Solutions: Regularization:  modify the error function that we minimize to penalize large weights. where f(w) grows larger as the weights grow larger and λ is the regularization strength Dropout:  keep a neuron active with some probability p or setting it to zero otherwise.  prevents the network from becoming too dependent on any one neuron. 17Initial Drawbacks (3)3. Multilayered ANNs need lots of labeled data  Solution: “pre-train” the network with features found automatically using unsupervised data  i.e. Automatic feature learning… (see next few slides)18Classic MLInputMotorbikes“Non”-MotorbikesLearningalgorithmFeature 1Feature 2Manual Extraction of Features (eg. edge detection, colors, texture,…)Classic ML, requires labeled data and hand-crafted features1. Needs expert knowledge2. Time-consuming      and expensive3. Does not         generalize to     other domainsSlide from Y. LeCun19Automatic Feature LearningInputMotorbikes“Non”-MotorbikesAutomatic Feature RepresentationLearningalgorithm“wheel”“handle”handlewheeleg. handle, wheel, … With Automatic Feature Learning:1. We feed the network the raw data (not feature-curated)2. The features are learned by the network3. Features learned can be re-used in similar tasks. 20Slide from Y. LeCunAutomatic Feature Learninghttps://www.strong.io/blog-images/movie-posters/Slide6.png 21Automatic Feature Learning Each layer learns more abstract features that are then combined / composed into higher-level features automatically Like the human brain …  has many layers of neurons which act as feature detectors detecting more and more abstract features as you go up E.g. to classify an image of a cat: Bottom Layers: Edge detectors, curves, corners straight lines Middle Layers: Fur patterns, eyes, ears Higher Layers: Body, head, legs Top Layer: Cat or DogDeep Learning = Machine learning algorithms based on learning multiple levels of representation / abstraction. – Y. Bengio22Automatic Feature Learning23What Types of Features? For image recognition  pixel -> edge -> texton -> motif -> part -> object For NLP character -> word ->  constituents -> clause -> sentence -> discourse For speech: sample  spectral band -> sound -> … phone -> phoneme -> word→Figure from Y LeCun 24Eg: Learning Image FeaturesFaces Cars Elephants ChairsActual images (pixels)25Learned object partsLearned edgesExamples of learned objects parts from object categoriesLearned objectsLearned features / representations can be used in  variety of OTHER classification tasks…  deep learning→Advantages of Unsupervised Feature Learning 26Advantages of Unsupervised Feature Learning  Much more unlabeled data available than labeled data: Eg. Websites, Books, Videos, Pictures Humans learn initially from unlabeled examples Eg. Babies learn to talk/recognize objects without labeled data As the features are learned in an unsupervised way from a different and larger dataset, less risk of over-fitting No need for manual feature engineering  These features are organized into multiple levels Each level creates new features from combinations of features from the level below Each level is more abstract than the ones below (hierarchy of features)27Today1. Motivation2. Feature Learning3. Early Training of Deep Neural Networks4. CNNs for Image Processing5. Deep Learning for NLP6. Conclusion28General Architecture of a Deep Network1. Unsupervised pre-training of neural network using unlabeled data aka. unsupervised learning of features2. Supervised training with labeled data using features learned from above with a standard classifier Eg. an ANN, SVM, … 29Training a Deep NNthen this layerfinally this layerthen this layerthen this layertrain this layer firstEACH of the (non-output) layers is trained to learn a representation of the data (eg. autoencoder) aka “pretraining the network with unsupervised data”Use pretrained representations to feed a regular ANN with a smaller set of labelled data. 30Autoencoders To learn a representation of the data, we can use:1. Deep Belief Networks  Mid 2000’s: Geoffrey Hinton trains a deep network by:  Stacking  Restricted Boltzmann Machines (RBM’s) on top of one another – deep belief network Training layer by layer on un-labeled data Then, using back propagation to fine tune weights on labeled data2. Autoencoders 2006: Yoshua Bengio et al. does something similar using auto-encoders instead of RBM’s Both are 2 layer neural networks that learn to model their inputs31Autoencoder The network is trained to output the input i.e. learn the identity function.  Trivial… unless, we impose constraints: Nb of units in layer 2 < nb of input units (learn compressed representation) OR Constrain layer 2 to be sparse (i.e. many connections are “disabled”)  x4x5x6+1layer 1 (input)layer 2x1x2x3x4x5x6x1x2x3+1layer 3 (output)a1a2a332 → Worksheet #6 (“Autoencoder”)Autoencoderx4x5x6+1input layerlayer 2x1x2x3x4x5x6x1x2x3+1a1a2a3Feedforward inputBackprop erroroutput layer33 → Worksheet #6 (“Autoencoder Activation”)Autoencoderx4x5x6+1input layerlayer 2x1x2x3x4x5x6x1x2x3+1a1a2a3output layer34AutoencoderNew (compressed) representation of the input to be fed to the next layeri.e. the encoded x x4x5x6+1layer 2x1x2x3a1a2a3 input layer 35x4x5x6+1x1x2x3+1a1a2a3+1b1b2b3Train parameters (weights)But keep bi’s sparse (ie. many zeros). Autoencodernew input layernew output layerFeedforward inputBackprop errora1a2a336x4x5x6+1x1x2x3+1a1a2a3+1b1b2b3Autoencodernew input layernew output layera1a2a337x4x5x6+1x1x2x3+1a1a2a3+1b1b2b3AutoencoderNew representation for the input to the next layeri.e. the encoded a 38x4x5x6+1x1x2x3+1a1a2a3+1b1b2b3+1c1c2c3Autoencodernew input layernew output layerTrain parameterssubject to ci’s being sparse. Feedforward inputBackprop errorb1b3b239x4x5x6+1x1x2x3+1a1a2a3+1b1b2b3+1c1c2c3Autoencodernew input layernew output layerb1b2b340x4x5x6+1x1x2x3+1a1a2a3+1b1b2b3+1c1c2c3New representation for input. Use [c1, c3, c3] as representation to feed to supervised learning algorithm (the last, supervised, layer).Autoencoder 41Training a Deep NNthen this layerfinally this layerthen this layerthen this layertrain this layer firstUse of unlabelled data to “pretrain” the networki.e. learn more and more abstract feature representationsUse pretrained representations to feed a regular ANN with a smaller set of labelled data. 42Many Types of Neural Networks  https://becominghuman.ai/cheat-sheets-for-ai-neural-networks-machine-learning-deep-learning-big-data-678c51b4b463 43Many Types of Deep Networks (con’t) https://becominghuman.ai/cheat-sheets-for-ai-neural-networks-machine-learning-deep-learning-big-data-678c51b4b463 44Today1. Motivation2. Feature Learning3. Early Training of Deep Neural Networks4. CNNs for Image Processing5. Conclusion45CNNs for Image Processinghttps://www.analyticsvidhya.com/blog/2017/06/architecture-of-convolutional-neural-networks-simplified-demystified/Image of a 4 in grey scaleValue = 0-> white …. 255->blackCNNs = Convolutional Neural Networks46CNNs for Image Processinghttps://www.analyticsvidhya.com/blog/2017/06/architecture-of-convolutional-neural-networks-simplified-demystified/ Standard input of the image in the ANN:1. 1 pixel = 1 input. 2. Eg. color image of 200x200x 3channels (RGB)        --> in a fully connected ANN, a neuron of the                 input layer has 200*200*3 = 120,000 weights       --> huge number of parameters, can easily             overfit2. We linearize the image ==> We lose spatial information…47Convolutional Layer Use a filter (aka kernel) that “convolves” on the image Filter = small weight matrix to learn  1 0.30.5 2219.2 23.9 147.2283.9 22.9 349.592.6 16.1 195.4139.6 20.4 377.7121.9 9.9 417.5223.8 13.6 341.4620.4 268.2 443.6486.1 731.2 73686 4 8 184252 3 8 4034 7 7 163105 2 3 6956 3 8 175126 1 2 178163 8 4 14222 222 74 180163 158 204 25348Convolutional Layer Use a filter (aka kernel) that “convolves” on the image Filter = small weight matrix to learn  1 0.30.5 2219.2 23.9 147.2283.9 22.9 349.592.6 16.1 195.4139.6 20.4 377.7121.9 9.9 417.5223.8 13.6 341.4620.4 268.2 443.6486.1 731.2 73686 4 8 184252 3 8 4034 7 7 163105 2 3 6956 3 8 175126 1 2 178163 8 4 14222 222 74 180163 158 204 25349Convolutional Layer Use a filter (aka kernel) that “convolves” on the image Filter = small weight matrix to learn  1 0.30.5 2219.2 23.9 147.2283.9 22.9 349.592.6 16.1 195.4139.6 20.4 377.7121.9 9.9 417.5223.8 13.6 341.4620.4 268.2 443.6486.1 731.2 73686 4 8 184252 3 8 4034 7 7 163105 2 3 6956 3 8 175126 1 2 178163 8 4 14222 222 74 180163 158 204 25350Convolutional Layer Use a filter (aka kernel) that “convolves” on the image Filter = small weight matrix to learn  1 0.30.5 2219.2 23.9 147.2283.9 22.9 349.592.6 16.1 195.4139.6 20.4 377.7121.9 9.9 417.5223.8 13.6 341.4620.4 268.2 443.6486.1 731.2 73686 4 8 184252 3 8 4034 7 7 163105 2 3 6956 3 8 175126 1 2 178163 8 4 14222 222 74 180163 158 204 25351Learn the Filtershttps://www.analyticsvidhya.com/blog/2017/06/architecture-of-convolutional-neural-networks-simplified-demystified/ The weight matrix (filter/kernel) behaves like a filter  The network learns the values of the filter(s) that activate when they “see” some visual feature that is useful to identify the object (the final classification) Ex. a horizontal line, a blotch of some color, a circle…  429 505 686 856261 792 412 640633 653 851 751608 913 713 6571 0 10 1 01 0 118 54 51 239 244 18855 121 75 78 95 8835 24 204 113 109 2213 154 104 235 25 13015 253 225 159 78 23368 85 180 214 245 052Convolution Hyper-parameters1. Stride2. Padding53Stride (7×7)          W (3×3) with stride =1          C  (5×5)   (7×7)          W (3×3) with stride =2          C  (3×3)  54 → Worksheet #6 (“CNN Activation Map”)Padding9 0 0 11 0 1 00 1 1 22 1 0 10 0 00 1 00 0 00 11 1filter should pick up high values surrounded by low values 9 not picked up ;-( 0 0 0 0 0 00 9 0 0 1 00 1 0 1 0 00 0 1 1 2 00 2 1 0 1 00 0 0 0 0 00 0 00 1 00 0 09 0 0 11 0 1 00 1 1 22 1 0 19 picked up ;-) 55Learn Several Filtershttps://www.analyticsvidhya.com/blog/2017/06/architecture-of-convolutional-neural-networks-simplified-demystified/ So we create 1 activation map per filter5x556Pooling Layerhttps://www.analyticsvidhya.com/blog/2017/06/architecture-of-convolutional-neural-networks-simplified-demystified/ Used to: To reduce the size of the activation maps So that we reduce the number of parameters of the network and hence avoid overfitting. Several strategies: Max pooling Average pooling …429 505 686 856261 792 412 640633 653 851 751608 913 713 657792 856913 851429 505 686 856261 792 412 640633 653 851 751608 913 713 657496.8 648.5701.8 74357 → Worksheet #6 (“Pooling Layer”)Architecture of a CNNhttps://www.analyticsvidhya.com/blog/2017/06/architecture-of-convolutional-neural-networks-simplified-demystified/ Stack: Convolutional Layers Pooling Layers Finish off with: A fully connected layer at the end for the final classification58Learning a Hierarchy of Featureshttps://www.analyticsvidhya.com/blog/2017/06/architecture-of-convolutional-neural-networks-simplified-demystified/59Example of a CNNhttp://cs231n.github.io/convolutional-networks/ 60Successful CNN Networkshttp://cs231n.github.io/convolutional-networks/ LeNet First successful applications of CNNs  Developed by Yann LeCun in the 1990’s used to read zip codes, digits, etc. AlexNet First work that popularized CNNs for computer vision developed by Alex Krizhevsky, Ilya Sutskever and Geoff Hinton (U. of Toronto) In 2012 significantly outperformed all teams at the ImageNet ILSVRC challenge61http://yann.lecun.com/exdb/lenet/http://vision.stanford.edu/teaching/cs231b_spring1415/slides/alexnet_tugce_kyunghee.pdfWhy now?1. Basic science Backpropagation did not work / overfitting…  now: developed method for training, better activation functions, better architectures….  Need for lots training data…  now: we have massive amounts + unsupervised pre-training2. GPU computing Neural networks take very very long to train… (days, weeks)  now: use of GPU’s which are optimized for very fast matrix multiplication3. Open Access to resources now : Access to DL methods, code and frameworks now : Fast turnaround from idea to implementation62History of AIhttps://www.researchgate.net/profile/Dubravko_Miljkovic/publication/268239364/figure/fig30/AS:394719407427587@147111http://www.cormix.info/images/RuleTreeExample.jpgRules written by experts (eg. linguistics, medical doctors,…)63https://www.researchgate.net/profile/Dubravko_Miljkovic/publication/268239364/figure/fig30/AS:394719407427587@147111https://www.researchgate.net/profile/Dubravko_Miljkovic/publication/268239364/figure/fig30/AS:394719407427587@147111History of AI Rules learns via the data ;-)But: features identified by the experts (eg. linguistics, medical doctors,…) 64History of AIRules AND features learned from the data65Conclusion Deep Learning is thriving ! vision image processing speech recognition natural language processing … Canada is a world leader in Deep Learning1. Montreal: (Bengio et al.)   MILA2. Toronto: (Hinton et al.)  Vector Institute3. Edmonton: (Sutton et al.) AMII 66https://mila.quebec/en/http://vectorinstitute.ai/https://www.amii.ca/