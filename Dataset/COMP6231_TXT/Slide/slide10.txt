Artificial Intelligence:  Natural Language Processing1Artificial Intelligence:Introduction to  Natural Language ProcessingMenu1. Introduction2. Bag of Word model3.  n-gram Language models4.  Linguistic features for NLP 2Languages Artificial Smaller vocabulary Simple syntactic structures Non-ambiguous Not tolerant to errors (ex. Syntax error) Natural Large and open vocabulary (new words everyday) Complex syntactic structures Very ambiguous Robust (ex. forgot a comma, a word… still OK) 3Question Answering: IBM’s Watson Won Jeopardy on February 16, 2011!4WILLIAM WILKINSON’S “AN ACCOUNT OF THE PRINCIPALITIES OFWALLACHIA AND MOLDOVIA”INSPIRED THIS AUTHOR’SMOST FAMOUS NOVELWho is Bram Stoker?(Dracula)Information ExtractionSubject: curriculum meetingDate: January 15, 2012To: Dan JurafskyHi Dan, we’ve now scheduled the curriculum meeting.It will be in Gates 159 tomorrow from 10:00-11:30.-Chris Create new Calendar entryEvent:  Curriculum mtgDate:   Jan-16-2012Start:   10:00amEnd:    11:30amWhere: Gates 159Information Extraction & Sentiment Analysisnice and compact to carry! since the camera is small and light, I won't need to carry around those heavy, bulky professional cameras either! the camera feels flimsy, is plastic and very light in weight you have to be very delicate in the handling of this camera6Size and weightAttributes: zoom affordability size and weight flash  ease of use✓✗✓slide from Olga Veksler (U. Western Ontario)Machine TranslationFully automatic7Helping human translatorsEnter Source Text:Translation from Stanford’s Phrasal:  这 不过 是 一 个 时间 的 问题 .This is only a matter of time.slide from Olga Veksler (U. Western Ontario)Where we are todayPart-of-speech (POS) taggingNamed entity recognition (NER)Sentiment analysismostly solved making good progress Good progress by Deep LearningSpam detectionLet’s go to Agra!Buy V1AGRA …✓✗Colorless   green   ideas   sleep   furiously.     ADJ         ADJ    NOUN  VERB      ADVEinstein met with UN officials in PrincetonPERSON              ORG                      LOCInformation extraction (IE)You’re invited to our dinner party, Friday May 27 at 8:30PartyMay 27addBest roast chicken in San Francisco!The waiter ignored us for 20 minutes.Machine translation (MT)The 13th Shanghai International Film Festival…第 13 届上海国际电影节开幕…Question answering (QA)Q. How effective is ibuprofen in reducing fever in patients with acute febrile illness?ParsingI can see Alcatraz from the window!ParaphraseXYZ acquired ABC yesterdayABC has been taken over by XYZSummarizationThe Dow Jones is upHousing prices roseEconomy is goodThe S&P500 jumpedCoreference resolutionCarter told Mubarak he shouldn’t run again.Word sense disambiguation (WSD)I need new batteries for my mouse.Dialog  Where is Citizen Kane playing in SF? Castro Theatre at 7:30. Do you want a ticket?slide from Olga Veksler (U. Western Ontario) Because it is ambiguous:1. The computer understands you as well as your mother understands you.2. The computer understands that you like (love) your mother.3. The computer understands you as well as it understands your mother.Why is NLP hard?“At last, a computer that understands you like your mother”        slide from Olga Veksler (U. Western Ontario)10Another Example of Ambiguity Even simple sentences are highly ambiguous “Get the cat with the gloves”slide from Olga Veksler (U. Western Ontario)11And Even More Examples of Ambiguity Iraqi Head Seeks Arms Ban on Nude Dancing on Governor’s Desk Juvenile Court to Try Shooting Defendant Teacher Strikes Idle Kids Kids Make Nutritious Snacks British Left Waffles on Falkland Islands Red Tape Holds Up New Bridges Bush Wins on Budget, but More Lies Ahead Hospitals are Sued by 7 Foot Doctors Stolen Painting Found by Tree Local HS Dropouts Cut in Halfslide from Olga Veksler (U. Western Ontario) Natural Language Processing= automatic processing of written texts1. Natural Language Understanding Input = text2. Natural Language Generation Output = text Speech Processing= automatic processing of speech1. Speech Recognition  Input = acoustic signal2. Speech Synthesis Output = acoustic signalNLP vs Speech Processing 12Remember these slides?13The Ancient Land of NLP (aka GOFAI)(circa A.D. 1950...mid 1980)14The Ancient Land of NLPSpeech Kingdom Village of CS & LinguistsInformation Retrieval ForestMachine Learning IslandRule-based NLP(circa A.D. 1950...mid 1980)15https://image.slidesharecdn.com/maryamsaihbani-161004094341/95/lefttoright-hierarchical-phrasebased-translation-and-its-application-in-simultaneous-speech-translation-maryam-siahbani-4-638.jpg?cb=1477004310Symbolic methods / Linguistic approach / Knowledge-rich approach• Cognitive approach• Rules are developed by hand in collaboration with linguists1st Invasion of NLP, from ML(mid 1980 – circa 2010)16The Land of Statistical NLPSpeech Kingdom City of CS & LinguistsInformation Retrieval ForestMachine Learning IslandStatistical NLP(mid 1980 – circa 2010)17Syntactic parsingPart-of-speech taggingstemmingtokenisation Decision treesStatistical methods / Machine Learning / Knowledge-poor method• Engineering Approach• Rules are developed automatically (using machine learning)• But the linguistic features are hand-engineered and fed to the ML model  • Applications: Information Retrieval, Predictive Text / Word Completion, Language Identification, Text Classification, Authorship Attribution...Neural networksNaïve Bayes classifierK-means clusteringFeature Extraction (designed by hand)Machine Learning Model ApplicationsStatistical NLP(mid 1980 – circa 2010)18Applicationslinguistic features are hand-engineered and fed to the ML model  2nd Invasion of NLP, by Deep Learning(circa 2010-today)19The Modern Land of Deep Language ProcessingSpeech Kingdom Metropolis of  Deep Language ProcessingInformation Retrieval ForestDeep Learning IslandDeep Language Processing(circa 2010-today)20Deep Neural Networks applied to NLP problems• Rules are developed automatically (using machine learning)• And the linguistic features are found automatically!ApplicationsMenu 211. Introduction2. Bag of word model3. n-gram Language Models4. Linguistic features for NLP22Bag-of-word Model (BOW) A simple model where word order is ignored used in many applications: NB spam filter seen in class a few weeks ago Information Retrieval (eg. google search) ... But has severe limits to understand meaning of text... Maybe we should take word order into account...Word Freq.Mary 2apples 1did 2eat 1John 1kill 1like 1not 1to 123Limits of BOW Model word order is ignored ==> meaning of text is lost. n-grams take [a bit of] word order into accountWord Freq.Mary 2apples 1did 2eat 1John 1kill 1like 1not 1to 1Mary did kill John.Mary did not like to eat apples.John did not kill Mary.Mary did like to eat apples.Mary did not like to kill John.Mary did eat apples....Menu 241. Introduction2. Bag of word model3. n-gram Language Models4. Linguistic features for NLP25n-gram Model An n-gram model is a probability distribution over sequences of events (grams/units/items) models the order of the events Used when the past sequence of events is a good indicator of the next event to occur in the sequence i.e. To predict the next event in a sequence of event  E.g.:  next move of player based on his/her past moves left right right up ... up? down? left? right? next base pair based on past DNA sequence  AGCTTCG ... A? G? C? T? next word based on past words  Hi dear, how are ... helicopter? laptop? you? magic?26What’s a Language Model? A Language model is a n-gram model over word/character sequences ie: events = words  or  events = character P(“I’d like a coffee with 2 sugars and milk”) ≈ 0.001 P(“I’d hike a toffee with 2 sugars and silk”) ≈ 0.000000001Applications of Language Models Speech Recognition Statistical Machine Translation Language Identification Spelling correction  He is trying to fine out.  He is trying to find out.  Optical character recognition / Handwriting recognition …27In Statistical Machine Translation Assume we translate from fr[foreign] to English  i.e.: (en|fr)Given: Foreign sentence - frFind: The most likely English sentence – en*S1: Translate that!S2: Translated this! S3: Eat your soup!S4…Translation modelLanguage modelAutomatic Language Identification… guess how that’s done?P(en) x en)|P(fr argmaxen*en3031“Shannon Game” (Shannon, 1951)“I am going to make a collect …” Predict the next word/character given the n-1 previous words/characters.https://en.wikipedia.org/wiki/Claude_Shannon321st approximation each word has an equal probability to follow any other with 100,000 words, the probability of each word at any given point is .00001  but some words are more frequent then others… “the” appears many more times than “rabbit” 332nd approximation: unigrams take into account the frequency of the word in some training corpus at any given point, “the”  is more probable than “rabbit” but does not take word order into account.  This is the bag of word approach.  “Just then, the white …” so the probability of a word also depends on the previous words (the history)P(wn |w1w2…wn-1)34n-grams “the large green ______ .” “mountain”? “tree”?  “Sue swallowed the large green ______ .” “pill”?  “broccoli”?   Knowing that Sue “swallowed” helps narrow down possibilities  i.e., going back 3 words before helps But, how far back do we look?35Bigrams first-order Markov models N-by-N matrix of probabilities/frequencies  N = size of the vocabulary we are usingP(wn|wn-1)1st word2nd word a aardvark aardwolf aback … zoophyte zucchini a 0 0 0 0 … 8 5 aardvark 0 0 0 0 … 0 0 aardwolf 0 0 0 0 … 0 0 aback 26 1 6 0 … 12 2 … … … … … … … … zoophyte 0 0 0 1 … 0 0 zucchini 0 0 0 3 … 0 0   36Trigrams second-order Markov models N-by-N-by-N matrix of probabilities/frequencies  N = size of the vocabulary we are usingP(wn|wn-1wn-2)1st word2nd word3rd word37Why use only bi- or tri-grams?  Markov approximation is still costlywith a 20 000 word vocabulary: bigram needs to store 400 million parameters trigram needs to store 8 trillion parameters using a language model > trigram is impractical38Building n-gram Models1. Data preparation:  Decide on training corpus Clean and tokenize How do we deal with sentence boundaries?  I eat.  I sleep.     (I eat) (eat I) (I sleep)  <s>I eat </s> <s> I sleep </s>  (<s> I) (I eat) (eat </s>) (<s> I) (I sleep) (sleep </s>)39Example 1:  in a training corpus, we have 10 instances of “come across” 8 times, followed by “as” 1 time, followed by “more” 1 time, followed by “a” so we have:     P(more | come across) = 0.1  P(a | come across) = 0.1  P(X | come across) = 0  where X ≠ “as”, “more”, “a”108across) C(comeas) across C(come  across) come |P(as 40Building n-gram Models2. Count words and build model Let C(w1...wn) be the frequency of n-gram w1...wn For bigrams: C(w1w2) is the frequency of the bigram w1w2 and C(w1) the frequency of w1:  P(w2|w1) = C(w1w2) / C(w1)3. Smooth your model (see later))...wC(w)...wC(w  )...ww|(wP1-n1n11-n1n → Worksheet #9 (“Language Model”)43Example 2:→ Worksheet #9 (“Sentence Probability”)Remember this slide...4445Some Adjustments product of probabilities… numerical underflow for long sentences so instead of multiplying the probs, we add the log of the probsP(I want to eat British food) = log(P(I|<s>)) + log(P(want|I)) + log(P(to|want)) + log(P(eat|to)) + log(P(British|eat)) + log(P(food|British))= log(.25) + log(.32) + log(.65) + log (.26) + log(.001) + log(.6)46Problem: Data Sparseness   What if a sequence never appears in training corpus? P(X)=0 “come across the men” --> prob = 0   “come across some men” --> prob = 0  “come across 3 men” --> prob = 0 The model assigns a probability of zero to unseen events …  probability of an n-gram involving unseen words will be zero! Solution: smoothing decrease the probability of previously seen events  so that there is a little bit of probability mass left over for previously unseen events  Remember this other slide...4748Add-one Smoothing Pretend we have seen every n-gram at least once  Intuitively: new_count(n-gram) = old_count(n-gram) + 1 The idea is to give a little bit of the probability space to unseen events49Add-one: Exampleunsmoothed bigram counts (frequencies):1st word2nd word Assume a vocabulary of 1616 (different) words V = {a, aardvark, aardwolf, aback, … , I, …, want,… to, …, eat, Chinese, …, food, …, lunch, …,                 zoophyte, zucchini} |V| = 1616 words And a total of N = 10,000 bigrams (~word instances) in the training corpus I want to eat Chinese food lunch … Total I 8 1087 0 13 0 0 0  C(I)=3437 want 3 0 786 0 6 8 6  C(want)=1215 to 3 0 10 860 3 0 12  C(to)=3256 eat 0 0 2 0 19 2 52  C(eat)=938 Chinese 2 0 0 0 0 120 1  C(Chinese)=213 food 19 0 17 0 0 0 0  C(food)=1506 lunch 4 0 0 0 0 1 0  C(lunch)=459 …         ...          ...          N=10,000   50Add-one: Exampleunsmoothed bigram counts:unsmoothed bigram conditional probabilities:1st word2nd word I want to eat Chinese food lunch … Total I 8 1087 0 13 0 0 0  C(I)=3437 want 3 0 786 0 6 8 6  C(want)=1215 to 3 0 10 860 3 0 12  C(to)=3256 eat 0 0 2 0 19 2 52  C(eat)=938 Chinese 2 0 0 0 0 120 1  C(Chinese)=213 food 19 0 17 0 0 0 0  C(food)=1506 lunch 4 0 0 0 0 1 0  C(lunch)=459 …                   N=10,000   → Worksheet #9 (“Corpus Probabilities”)52Add-one, more formallyN: size of the corpus    i.e. number of n-gram tokens in training corpus B: number of "bins"    i.e. number of different n-gram types    i.e. number of cells in the matrix    e.g. for bigrams, it's (size of the vocabulary)2B  N1  )w w (w C  )w w (wP n1 2n21Add153Add-one: Example (con’t)add-one smoothed bigram counts:add-one bigram conditional probabilities: I want to eat Chinese food lunch … Total I 8   9 1087  1088 1 14 1 1 1  3437   C(I) + |V| = 5053 want 3  4 1 787 1 7 9 7  C(want) + |V| = 2831 to 4 1 11 861 4 1 13  C(to) + |V| = 4872 eat 1 1 23 1 20 3 53  C(eat) + |V| = 2554 Chinese 3 1 1 1 1 121 2  C(Chinese) + |V| = 1829 food 20 1 18 1 1 1 1  C(food) + |V| = 3122 lunch 5 1 1 1 1 2 1  C(lunch) + |V| = 2075 …         total = 10,000 N+|V|2 = 10,000 + (1616)2 = 2,621,456    I want to eat Chinese food lunch … I .0018 (9/5053) .215 .00019 .0028  .00019 .00019 .00019  want .0014 .00035 .278 .00035 .0025 .0031 .00247  to .00082 .0002 .00226 .1767 .00082 .0002 .00267  eat .00039 .00039 .0009 .00039 .0078 .0012 .0208  …           → Worksheet #9 (“Smoothing”)55Add-delta Smoothing every previously unseen n-gram is given a low probability but there are so many of them that too much probability mass is given to unseen events instead of adding 1, add some other (smaller) positive value 𝛿  most widely used value for 𝛿 = 0.5 better than add-one, but still…B   N  )w w (w C  )w w (wP n1 2n21AddD56Factors of Training Corpus Size:  the more, the better but after a while, not much improvement… bigrams (characters) after 100’s million words trigrams (characters) after some billions of words Genre (adaptation): training on cooking recipes and testing on aircraft maintenance manuals57Example: Language Identification hypothesis: texts that resemble each other (same author, same language) share similar character/word sequences   In English character sequence “ing”  is more probable than in French   Training phase:  construction of the language model  with pre-classified documents (known language/author) Testing phase:  apply language model to unknown textAutomatic Language Identification… 58Example: Language Identification bigram of characters  characters = 26 letters (case insensitive) possible variations: case sensitivity, punctuation, beginning/end of sentence marker, …591. Train a character-based language model for Italian:2. Train a character-based language model for Spanish:3. Given a unknown sentence “che bella cosa”  is it in Italian or in Spanish?P(“che bella cosa”) with the Italian LMP(“che bella cosa”) with the Spanish LM4. Highest probability  language of sentence→Example: Language Identification A B C D … Y Z A 0.0014 0.0014 0.0014 0.0014 … 0.0014 0.0014 B 0.0014 0.0014 0.0014 0.0014 … 0.0014 0.0014 C 0.0014 0.0014 0.0014 0.0014 … 0.0014 0.0014 D 0.0042 0.0014 0.0014 0.0014 … 0.0014 0.0014 E 0.0097 0.0014 0.0014 0.0014 … 0.0014 0.0014 … … … … … … … 0.0014 Y 0.0014 0.0014 0.0014 0.0014 … 0.0014 0.0014 Z 0.0014 0.0014 0.0014 0.0014 0.0014 0.0014 0.0014    A B C D … Y Z A 0.0014 0.0014 0.0014 0.0014 … 0.0014 0.0014 B 0.0014 0.0014 0.0014 0.0014 … 0.0014 0.0014 C 0.0014 0.0014 0.0014 0.0014 … 0.0014 0.0014 D 0.0042 0.0014 0.0014 0.0014 … 0.0014 0.0014 E 0.0097 0.0014 0.0014 0.0014 … 0.0014 0.0014 … … … … … … … 0.0014 Y 0.0014 0.0014 0.0014 0.0014 … 0.0014 0.0014 Z 0.0014 0.0014 0.0014 0.0014 0.0014 0.0014 0.0014   Google’s Web 1T 5-gram model 5-grams generated from 1 trillion words 24 GB compressed  Number of tokens: 1,024,908,267,229  Number of sentences: 95,119,665,584  Number of unigrams: 13,588,391  Number of bigrams: 314,843,401  Number of trigrams: 977,069,902  Number of fourgrams: 1,313,818,354  Number of fivegrams: 1,176,470,663 See discussion: http://googleresearch.blogspot.com/2006/08/all-our-n-gram-are-belong-to-you.html See Google Ngram Viewer: http://en.wikipedia.org/wiki/Google_Ngram_Viewer60http://googleresearch.blogspot.com/2006/08/all-our-n-gram-are-belong-to-you.htmlhttp://en.wikipedia.org/wiki/Google_Ngram_ViewerProblem with n-grams Natural language is not linear .... there may be long-distance dependencies. Syntactic dependencies The man next to the large oak tree near … is tall. The men next to the large oak tree near … are tall. Semantic dependencies The bird next to the large oak tree near … flies rapidly. The man next to the large oak tree near … talks rapidly. World knowledge Michael Jackson, who was featured in ..., is buried in California.  Michael Bublé, who was featured in ..., is living in California.  ... More complex models of language are needed to handle such dependencies.61Menu 621. Introduction2. Bag of word model3. n-gram models4. Linguistic features for NLPLinguistic features used for what?63Applicationslinguistic features are hand-engineered and fed to the ML model  64Stages of NLUsource: Luger (2005)67Stages of NLUsource: Luger (2005)Parsing (Syntax):  What words are available in a language?  gfiioudd  / table How to arrange words together? the rose is red /  red the rose is68Syntactic Parsing1. Assign the right part of speech (NOUN, VERB, …) to individual words in a text2. Determine how words are put together to form correct sentences The/DET rose/NOUN is/VERB red/ADJ.   Is/VERB red/ADJ the/DET rose/NOUN.69English Parts-of-Speech Open (lexical) class words  new words can be added easily nouns, main verbs, adjectives, adverbs some languages do not have all these categories Closed (functional) class words generally function/grammatical words  aka stop words ex. the, in, and, over, beyond… relatively fixed membership prepositions, determiners, pronouns, conjunctions, …Smurf talk on youtube:https://www.youtube.com/watch?v=7BPx-vl8G00→ Worksheet #9 (“Part-of-Speech Tagging”)https://www.youtube.com/watch?v=7BPx-vl8G00https://www.youtube.com/watch?v=7BPx-vl8G0070Syntax  How parts-of-speech are organised into larger syntactic constituents  Main Constituents: S: sentence              The boy is happy. NP: noun phrase             the little boy from Paris, Sam Smith, I,  VP: verb phrase        eat an apple, sing, leave Paris in the night  PP: prepositional phrase   in the morning, about my ticket AdjP: adjective phrase    really funny, rather clear AdvP: adverb phrase        slowly, really slowly71A Parse Tree a tree representation of the application of the grammar to a specific sentence.72a CFG consists of set of non-terminal symbols  constituents & parts-of-speech S, NP, VP, PP, D, N, V, ... set of terminal symbols  words & punctuation cat, mouse, nurses, eat, ... a non-terminal designated as the starting symbol  sentence S a set of re-write rules  having a single non-terminal on the LHS and one or more terminal or non-terminal in the RHS S --> NP VP  NP --> Pro  NP --> PN  NP --> D N73An Example Lexicon:N --> flight | trip | breeze | morning // nounV --> is | prefer | like  // verbAdj --> direct | cheapest | first // adjectivePro --> me | I | you | it // pronounPN --> Chicago | United | Los Angeles // proper nounD --> the | a | this // determinerPrep --> from | to | in // prepositionConj --> and | or | but // conjunction Grammar:S --> NP VP // I + prefer UnitedNP --> Pro | PN | D N | D Adj N   // I, Chicago, the morningVP --> V | V NP | V NP PP   // is, prefer + United, PP --> Prep NP   // to Chicago, to I ??→ Worksheet #9 (“Parsing”)74Parsing parsing:  goal:  assign syntactic structures to a sentence  result:  (set of) parse trees we need: a grammar:  description of the language constructions a parsing strategy:  how the syntactic analysis are to be computed 75Parsing Strategies parsing is seen as a search problem through the space of all possible parse trees bottom-up (data-directed): words --> grammar top-down (goal-directed): grammar --> words  breadth-first: compute all paths in parallel depth-first: exhaust 1 path before considering another Heuristic search76Example: John ate the cat Bottom-up parsing / breadth first1. John ate the cat2. PN ate the cat3. PN V the cat4. PN V ART cat5. PN V ART N6. NP V ART N7. NP V NP8. NP VP9. S Top-down parsing / depth first1. S2. NP VP3. PN VP4. John VP5. John V NP6. John ate NP 7. John ate ART N8. John ate the N9. John ate the cat77Depth-first vs Breadth-first the cat eats the mouse. depth-first: exhaust 1 path before considering another breadth-first:  compute 1 level at a time Heuristic search:  e.g. preference to shorter rulesGrammar:(1) S --> NP VP(2) S --> VP(3) S --> Aux NP VP (4) NP --> Det N PP(5) NP -- > Det N(6) PP -- > Prep N…Lexicon:(10) Det --> the(11) N --> cat(12) VB --> eats…S   NP-VP       VP   Aux-NP-VP  Det-N-PP Det-N …  the  cat Prep-NP   78Summary of Parsing Strategies Depth First Breath First Heuristic Search Top down    Bottom up      84Stages of NLUsource: Luger (2005)Semantic interpretation:  Lexical Semantics : What is the meaning/semantic relations between individual words? Chair:  person?  Furniture? Compositional Semantics: What is the meaning of phrases and sentences?The chair’s leg is broken85Semantic Interpretation Map sentences to some representation of its meaning e.g., logics, knowledge graph, embedding…1. Lexical Semantics  i.e., Meaning of individual words2. Compositional Semantics i.e., Meaning of combination of words86Lexical Semantics ie. The meaning of individual words A word may denote different things (ex. chair) The meaning/sense of words is not clear-cut E.g. Overlapping of word senses across languageslegpatteétapejambe piedanimaljourneyhuman chair87Word Sense Disambiguation (WSD) Determining which sense of a word is used in a specific sentence I went to the bank of Montreal and deposited 50$. I went to the bank of the river and dangled my feet.88 WSD can be viewed as typical classification problem use machine learning techniques (ex. Naïve Bayes classifier, decision tree) to train a system that learns a classifier (a function f) to assign to unseen examples one of a fixed number of senses (categories) Input:  Target word: The word to be disambiguated  Features?  Output:  Most likely sense of the wordWSD as a Classification Problem89Features for WSD  intuition:  sense of a word depends on the sense of surrounding words ex: bass = fish, musical instrument, ... So use a window of words around the target word as featuresSurrounding words Most probable sense …river… fish …violin… instrument …salmon… fish …play… instrument …player… instrument …striped… fish   90Features for WSD Take a window of n words around the target word Encode information about the words around the target word An electric guitar and bass player stand off to one side, not really part of the scene, just as a sort of nod to gringo expectations perhaps.91Naïve Bayes WSD Goal: choose the most probable sense s* for a word given a vector V of surrounding words Feature vector V contains:  Features: words [fishing, big, sound, player, fly, rod, …] Value: frequency of these words in a window before & after the target word [0, 0, 0, 2, 1, 0, …] Bayes decision rule:  s* = argmaxsk P(sk|V)  where: S is the set of possible senses for the target word sk is a sense in S V is the feature vector92 Training a Naïve Bayes classifier = estimating P(vj|sk) and P(sk) from a sense-tagged training corpus= finding the most likely sense kNaïve Bayes WSDNumber of occurrences of feature j over the total number of features appearing in windows of SkNumber of  occurrences of sense k over number of all occurrences of ambiguous wordn1jkjk  s)s|P(v log  )P(s logargmaxs*k  )s,count(v )s,count(v  )s|P(vtktkjkj )count(word )count(s  )P(s kk 93Example Training corpus (context window = ±3 words):…Today the World Bank/BANK1 and partners are calling for greater relief……Welcome to the Bank/BANK1 of America the nation's leading financial institution… …Welcome to America's Job Bank/BANK1 Visit our site and……Web site of the European Central Bank/BANK1 located in Frankfurt……The Asian Development Bank/BANK1 ADB a multilateral development finance……lounging against verdant banks/BANK2 carving out the...…for swimming, had warned her off the banks/BANK2 of the Potomac. Nobody... Training: P(the|BANK1) = 5/30 P(the|BANK2) = 3/12 P(world|BANK1) = 1/30 P(world|BANK2) = 0/12 P(and|BANK1) = 1/30 P(and|BANK2) = 0/12 … …  P(off|BANK1) = 0/30 P(off|BANK2) = 1/12 P(Potomac|BANK1) = 0/30 P(Potomac|BANK2) = 1/12 P(BANK1) = 5/7 P(BANK2) = 2/7  Disambiguation: “I lost my left shoe on the banks of the river Nile.” Score(BANK1)=log(5/7) + log(P(shoe|BANK1))+log(P(on|BANK1))+log(P(the|BANK1)) … Score(BANK2)=log(2/7) + log(P(shoe|BANK2)+log(P(on|BANK2))+log(P(the|BANK2)) …BANK1 BANK294Example (with add 0.5 smoothing) Training corpus (context window = ±3 words):…Today the World Bank/BANK1 and partners are calling for greater relief……Welcome to the Bank/BANK1 of America the nation's leading financial institution… …Welcome to America's Job Bank/BANK1 Visit our site and……Web site of the European Central Bank/BANK1 located in Frankfurt……The Asian Development Bank/BANK1 ADB a multilateral development finance……lounging against verdant banks/BANK2 carving out the...…for swimming, had warned her off the banks/BANK2 of the Potomac. Nobody... Assume V = 50 Training: P(the|BANK1) = (5+.5) / (30+.5V) P(the|BANK2) = (3+.5) / (12 + .5V) P(world|BANK1) = (1+.5) / 55 P(world|BANK2) = (0+.5) / 37 P(and|BANK1) = (1+.5) / 55  P(and|BANK2) = (0+.5) / 37 … P(off|BANK1) = (0+.5) / 55 P(off|BANK2) = (1+.5) / 37 P(Potomac|BANK1) = (0+.5) / 55 P(Potomac|BANK2) = (1+.5) / 37 P(BANK1) = 5/7 P(BANK2) = 2/7 Disambiguation: “I lost my left shoe on the banks of the river Nile.” Score(BANK1)=log(5/7) + log(P(shoe|BANK1))+log(P(on|BANK1))+log(P(the|BANK1)) … Score(BANK2)=log(2/7) + log(P(shoe|BANK2))+log(P(on|BANK2))+log(P(the|BANK2)) …→ Worksheet #9 (“Word Sense Disambiguation”)104Stages of NLUsource: Luger (2005) Discourse AnalysisHow to relate the meaning of sentences to surrounding sentences?I have to go to the store.  I need butter.I have to go to the university.  I need butter. PragmaticsHow people use language in a social environment?Do you have a child?    Do you have a quarter? World Knowledge How knowledge about the world (history, facts, …) modifies our understanding of text?Bill Gates passed away last night.105Using World Knowledge Using our general knowledge of the world to interpret a sentence/discourse E.g.: The trophy would not fit in the brown suitcase because ...     ... it was too big.     ... it was too small.The professor sent the student to see the principal because……he wanted to see him.…he was throwing paper balls in class.…he could not take it anymore. Ex: Silence of the lambs…Current Research area: see Winograd Schema Challengehttps://www.youtube.com/watch?v=sbJ89LFheTshttps://en.wikipedia.org/wiki/Winograd_Schema_Challengehttps://en.wikipedia.org/wiki/Winograd_Schema_Challenge106Summary of NLUsource: Luger (2005)Discourse AnalysisPragmaticsWorld Knowledge Lexical SemanticsCompositional SemanticsSyntactic ParsingRemember these slides?108(next lecture!)