Artificial Intelligence:  Natural Language Processing1Artificial Intelligence:Introduction to  Natural Language ProcessingMenu1. Introduction2. Bag of Word model3.  n-gram Language models4.  Linguistic features for NLPï® 2Languagesï® Artificialï± Smaller vocabularyï± Simple syntactic structuresï± Non-ambiguousï± Not tolerant to errors (ex. Syntax error)ï® Naturalï± Large and open vocabulary (new words everyday)ï± Complex syntactic structuresï± Very ambiguousï± Robust (ex. forgot a comma, a wordâ€¦ still OK)ï® 3Question Answering: IBMâ€™s Watsonï® Won Jeopardy on February 16, 2011!4WILLIAM WILKINSONâ€™S â€œAN ACCOUNT OF THE PRINCIPALITIES OFWALLACHIA AND MOLDOVIAâ€INSPIRED THIS AUTHORâ€™SMOST FAMOUS NOVELWho is Bram Stoker?(Dracula)Information ExtractionSubject: curriculum meetingDate: January 15, 2012To: Dan JurafskyHi Dan, weâ€™ve now scheduled the curriculum meeting.It will be in Gates 159 tomorrow from 10:00-11:30.-Chris Create new Calendar entryEvent:  Curriculum mtgDate:   Jan-16-2012Start:   10:00amEnd:    11:30amWhere: Gates 159Information Extraction & Sentiment Analysisnice and compact to carry! since the camera is small and light, I won't need to carry around those heavy, bulky professional cameras either! the camera feels flimsy, is plastic and very light in weight you have to be very delicate in the handling of this camera6Size and weightAttributes: zoom affordability size and weight flash  ease of useâœ“âœ—âœ“slide from Olga Veksler (U. Western Ontario)Machine TranslationFully automatic7Helping human translatorsEnter Source Text:Translation from Stanfordâ€™s Phrasal:Â  è¿™ ä¸è¿‡ æ˜¯ ä¸€ ä¸ª æ—¶é—´ çš„ é—®é¢˜ .This is only a matter of time.slide from Olga Veksler (U. Western Ontario)Where we are todayPart-of-speech (POS) taggingNamed entity recognition (NER)Sentiment analysismostly solved making good progress Good progress by Deep LearningSpam detectionLetâ€™s go to Agra!Buy V1AGRA â€¦âœ“âœ—Colorless   green   ideas   sleep   furiously.     ADJ         ADJ    NOUN  VERB      ADVEinstein met with UN officials in PrincetonPERSON              ORG                      LOCInformation extraction (IE)Youâ€™re invited to our dinner party, Friday May 27 at 8:30PartyMay 27addBest roast chicken in San Francisco!The waiter ignored us for 20 minutes.Machine translation (MT)The 13th Shanghai International Film Festivalâ€¦ç¬¬ 13 å±Šä¸Šæµ·å›½é™…ç”µå½±èŠ‚å¼€å¹•â€¦Question answering (QA)Q. How effective is ibuprofen in reducing fever in patients with acute febrile illness?ParsingI can see Alcatraz from the window!ParaphraseXYZ acquired ABC yesterdayABC has been taken over by XYZSummarizationThe Dow Jones is upHousing prices roseEconomy is goodThe S&P500 jumpedCoreference resolutionCarter told Mubarak he shouldnâ€™t run again.Word sense disambiguation (WSD)I need new batteries for my mouse.Dialog  Where is Citizen Kane playing in SF? Castro Theatre at 7:30. Do you want a ticket?slide from Olga Veksler (U. Western Ontario)ï® Because it is ambiguous:1. The computer understands you as well as your mother understands you.2. The computer understands that you like (love) your mother.3. The computer understands you as well as it understands your mother.Why is NLP hard?â€œAt last, a computer that understands you like your motherâ€        slide from Olga Veksler (U. Western Ontario)10Another Example of Ambiguityï± Even simple sentences are highly ambiguousï± â€œGet the cat with the glovesâ€slide from Olga Veksler (U. Western Ontario)11And Even More Examples of Ambiguityï® Iraqi Head Seeks Armsï® Ban on Nude Dancing on Governorâ€™s Deskï® Juvenile Court to Try Shooting Defendantï® Teacher Strikes Idle Kidsï® Kids Make Nutritious Snacksï® British Left Waffles on Falkland Islandsï® Red Tape Holds Up New Bridgesï® Bush Wins on Budget, but More Lies Aheadï® Hospitals are Sued by 7 Foot Doctorsï® Stolen Painting Found by Treeï® Local HS Dropouts Cut in Halfslide from Olga Veksler (U. Western Ontario)ï® Natural Language Processing= automatic processing of written texts1. Natural Language Understandingï± Input = text2. Natural Language Generationï± Output = textï® Speech Processing= automatic processing of speech1. Speech Recognition ï± Input = acoustic signal2. Speech Synthesisï± Output = acoustic signalNLP vs Speech Processingï® 12Remember these slides?13The Ancient Land of NLP (aka GOFAI)(circa A.D. 1950...mid 1980)14The Ancient Land of NLPSpeech Kingdom Village of CS & LinguistsInformation Retrieval ForestMachine Learning IslandRule-based NLP(circa A.D. 1950...mid 1980)15https://image.slidesharecdn.com/maryamsaihbani-161004094341/95/lefttoright-hierarchical-phrasebased-translation-and-its-application-in-simultaneous-speech-translation-maryam-siahbani-4-638.jpg?cb=1477004310Symbolic methods / Linguistic approach / Knowledge-rich approachâ€¢ Cognitive approachâ€¢ Rules are developed by hand in collaboration with linguists1st Invasion of NLP, from ML(mid 1980 â€“ circa 2010)16The Land of Statistical NLPSpeech Kingdom City of CS & LinguistsInformation Retrieval ForestMachine Learning IslandStatistical NLP(mid 1980 â€“ circa 2010)17Syntactic parsingPart-of-speech taggingstemmingtokenisation Decision treesStatistical methods / Machine Learning / Knowledge-poor methodâ€¢ Engineering Approachâ€¢ Rules are developed automatically (using machine learning)â€¢ But the linguistic features are hand-engineered and fed to the ML model  â€¢ Applications: Information Retrieval, Predictive Text / Word Completion, Language Identification, Text Classification, Authorship Attribution...Neural networksNaÃ¯ve Bayes classifierK-means clusteringFeature Extraction (designed by hand)Machine Learning Model ApplicationsStatistical NLP(mid 1980 â€“ circa 2010)18Applicationslinguistic features are hand-engineered and fed to the ML model  2nd Invasion of NLP, by Deep Learning(circa 2010-today)19The Modern Land of Deep Language ProcessingSpeech Kingdom Metropolis of  Deep Language ProcessingInformation Retrieval ForestDeep Learning IslandDeep Language Processing(circa 2010-today)20Deep Neural Networks applied to NLP problemsâ€¢ Rules are developed automatically (using machine learning)â€¢ And the linguistic features are found automatically!ApplicationsMenuï® 211. Introduction2. Bag of word model3. n-gram Language Models4. Linguistic features for NLP22Bag-of-word Model (BOW)ï® A simple model where word order is ignoredï® used in many applications:ï± NB spam filter seen in class a few weeks agoï± Information Retrieval (eg. google search)ï± ...ï® But has severe limits to understand meaning of text...ï® Maybe we should take word order into account...Word Freq.Mary 2apples 1did 2eat 1John 1kill 1like 1not 1to 123Limits of BOW Modelï® word order is ignored ==> meaning of text is lost.ï® n-grams take [a bit of] word order into accountWord Freq.Mary 2apples 1did 2eat 1John 1kill 1like 1not 1to 1Mary did kill John.Mary did not like to eat apples.John did not kill Mary.Mary did like to eat apples.Mary did not like to kill John.Mary did eat apples....Menuï® 241. Introduction2. Bag of word model3. n-gram Language Models4. Linguistic features for NLP25n-gram Modelï® An n-gram model is a probability distribution over sequences of events (grams/units/items)ï® models the order of the eventsï® Used when the past sequence of events is a good indicator of the next event to occur in the sequenceï® i.e. To predict the next event in a sequence of event ï® E.g.: ï± next move of player based on his/her past movesï® left right right up ... up? down? left? right?ï± next base pair based on past DNA sequence ï® AGCTTCG ... A? G? C? T?ï± next word based on past words ï® Hi dear, how are ... helicopter? laptop? you? magic?26Whatâ€™s a Language Model?ï® A Language model is a n-gram model over word/character sequencesï® ie: events = words  or  events = characterï® P(â€œIâ€™d like a coffee with 2 sugars and milkâ€) â‰ˆ 0.001ï® P(â€œIâ€™d hike a toffee with 2 sugars and silkâ€) â‰ˆ 0.000000001Applications of Language Modelsï® Speech Recognitionï® Statistical Machine Translationï® Language Identificationï® Spelling correction ï± He is trying to fine out. ï± He is trying to find out. ï® Optical character recognition / Handwriting recognitionï® â€¦27In Statistical Machine Translationï® Assume we translate from fr[foreign] to English  i.e.: (en|fr)Given: Foreign sentence - frFind: The most likely English sentence â€“ en*S1: Translate that!S2: Translated this! S3: Eat your soup!S4â€¦Translation modelLanguage modelAutomatic Language Identificationâ€¦ guess how thatâ€™s done?P(en) x en)|P(fr argmaxen*enï€½3031â€œShannon Gameâ€ (Shannon, 1951)â€œI am going to make a collect â€¦â€ï® Predict the next word/character given the n-1 previous words/characters.https://en.wikipedia.org/wiki/Claude_Shannon321st approximationï® each word has an equal probability to follow any otherï± with 100,000 words, the probability of each word at any given point is .00001 ï® but some words are more frequent then othersâ€¦ï® â€œtheâ€ appears many more times than â€œrabbitâ€ 332nd approximation: unigramsï® take into account the frequency of the word in some training corpusï± at any given point, â€œtheâ€  is more probable than â€œrabbitâ€ï® but does not take word order into account.  This is the bag of word approach. ï± â€œJust then, the white â€¦â€ï® so the probability of a word also depends on the previous words (the history)P(wn |w1w2â€¦wn-1)34n-gramsï® â€œthe large green ______ .â€ï± â€œmountainâ€? â€œtreeâ€? ï® â€œSue swallowed the large green ______ .â€ï± â€œpillâ€?  â€œbroccoliâ€?  ï® Knowing that Sue â€œswallowedâ€ helps narrow down possibilities ï® i.e., going back 3 words before helpsï® But, how far back do we look?35Bigramsï® first-order Markov modelsï® N-by-N matrix of probabilities/frequencies ï® N = size of the vocabulary we are usingP(wn|wn-1)1st word2nd word a aardvark aardwolf aback â€¦ zoophyte zucchini a 0 0 0 0 â€¦ 8 5 aardvark 0 0 0 0 â€¦ 0 0 aardwolf 0 0 0 0 â€¦ 0 0 aback 26 1 6 0 â€¦ 12 2 â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ zoophyte 0 0 0 1 â€¦ 0 0 zucchini 0 0 0 3 â€¦ 0 0   36Trigramsï® second-order Markov modelsï® N-by-N-by-N matrix of probabilities/frequencies ï® N = size of the vocabulary we are usingP(wn|wn-1wn-2)1st word2nd word3rd word37Why use only bi- or tri-grams? ï® Markov approximation is still costlywith a 20 000 word vocabulary:ï± bigram needs to store 400 million parametersï± trigram needs to store 8 trillion parametersï± using a language model > trigram is impractical38Building n-gram Models1. Data preparation: ï± Decide on training corpusï± Clean and tokenizeï± How do we deal with sentence boundaries? ï® I eat.  I sleep.    ï± (I eat) (eat I) (I sleep) ï® <s>I eat </s> <s> I sleep </s> ï± (<s> I) (I eat) (eat </s>) (<s> I) (I sleep) (sleep </s>)39Example 1: ï® in a training corpus, we have 10 instances of â€œcome acrossâ€ï± 8 times, followed by â€œasâ€ï± 1 time, followed by â€œmoreâ€ï± 1 time, followed by â€œaâ€ï® so we have:  ï±  ï± P(more | come across) = 0.1 ï± P(a | come across) = 0.1 ï± P(X | come across) = 0  where X â‰  â€œasâ€, â€œmoreâ€, â€œaâ€108across) C(comeas) across C(come  across) come |P(as ï€½ï€½40Building n-gram Models2. Count words and build modelï± Let C(w1...wn) be the frequency of n-gram w1...wnï± For bigrams: C(w1w2) is the frequency of the bigram w1w2 and C(w1) the frequency of w1: ï¬ P(w2|w1) = C(w1w2) / C(w1)3. Smooth your model (see later))...wC(w)...wC(w  )...ww|(wP1-n1n11-n1n ï€½â†’ Worksheet #9 (â€œLanguage Modelâ€)43Example 2:â†’ Worksheet #9 (â€œSentence Probabilityâ€)Remember this slide...4445Some Adjustmentsï® product of probabilitiesâ€¦ numerical underflow for long sentencesï® so instead of multiplying the probs, we add the log of the probsP(I want to eat British food) = log(P(I|<s>)) + log(P(want|I)) + log(P(to|want)) + log(P(eat|to)) + log(P(British|eat)) + log(P(food|British))= log(.25) + log(.32) + log(.65) + log (.26) + log(.001) + log(.6)46Problem: Data Sparseness  ï® What if a sequence never appears in training corpus? P(X)=0ï± â€œcome across the menâ€ --> prob = 0  ï± â€œcome across some menâ€ --> prob = 0 ï± â€œcome across 3 menâ€ --> prob = 0ï® The model assigns a probability of zero to unseen events â€¦ ï® probability of an n-gram involving unseen words will be zero!ï® Solution: smoothingï± decrease the probability of previously seen events ï± so that there is a little bit of probability mass left over for previously unseen events  Remember this other slide...4748Add-one Smoothingï® Pretend we have seen every n-gram at least once ï® Intuitively:ï± new_count(n-gram) = old_count(n-gram) + 1ï® The idea is to give a little bit of the probability space to unseen events49Add-one: Exampleunsmoothed bigram counts (frequencies):1st word2nd wordï® Assume a vocabulary of 1616 (different) wordsï± V = {a, aardvark, aardwolf, aback, â€¦ , I, â€¦, want,â€¦ to, â€¦, eat, Chinese, â€¦, food, â€¦, lunch, â€¦,                 zoophyte, zucchini}ï± |V| = 1616 wordsï® And a total of N = 10,000 bigrams (~word instances) in the training corpus I want to eat Chinese food lunch â€¦ Total I 8 1087 0 13 0 0 0  C(I)=3437 want 3 0 786 0 6 8 6  C(want)=1215 to 3 0 10 860 3 0 12  C(to)=3256 eat 0 0 2 0 19 2 52  C(eat)=938 Chinese 2 0 0 0 0 120 1  C(Chinese)=213 food 19 0 17 0 0 0 0  C(food)=1506 lunch 4 0 0 0 0 1 0  C(lunch)=459 â€¦         ...          ...          N=10,000   50Add-one: Exampleunsmoothed bigram counts:unsmoothed bigram conditional probabilities:1st word2nd word I want to eat Chinese food lunch â€¦ Total I 8 1087 0 13 0 0 0  C(I)=3437 want 3 0 786 0 6 8 6  C(want)=1215 to 3 0 10 860 3 0 12  C(to)=3256 eat 0 0 2 0 19 2 52  C(eat)=938 Chinese 2 0 0 0 0 120 1  C(Chinese)=213 food 19 0 17 0 0 0 0  C(food)=1506 lunch 4 0 0 0 0 1 0  C(lunch)=459 â€¦                   N=10,000   â†’ Worksheet #9 (â€œCorpus Probabilitiesâ€)52Add-one, more formallyN: size of the corpus    i.e. number of n-gram tokens in training corpus B: number of "bins"    i.e. number of different n-gram types    i.e. number of cells in the matrix    e.g. for bigrams, it's (size of the vocabulary)2B  N1  )w w (w C  )w w (wP n1 2n21Add1ï€«ï€«ï‚¼ï€½ï‚¼53Add-one: Example (conâ€™t)add-one smoothed bigram counts:add-one bigram conditional probabilities: I want to eat Chinese food lunch â€¦ Total I 8   9 1087  1088 1 14 1 1 1  3437   C(I) + |V| = 5053 want 3  4 1 787 1 7 9 7  C(want) + |V| = 2831 to 4 1 11 861 4 1 13  C(to) + |V| = 4872 eat 1 1 23 1 20 3 53  C(eat) + |V| = 2554 Chinese 3 1 1 1 1 121 2  C(Chinese) + |V| = 1829 food 20 1 18 1 1 1 1  C(food) + |V| = 3122 lunch 5 1 1 1 1 2 1  C(lunch) + |V| = 2075 â€¦         total = 10,000 N+|V|2 = 10,000 + (1616)2 = 2,621,456    I want to eat Chinese food lunch â€¦ I .0018 (9/5053) .215 .00019 .0028  .00019 .00019 .00019  want .0014 .00035 .278 .00035 .0025 .0031 .00247  to .00082 .0002 .00226 .1767 .00082 .0002 .00267  eat .00039 .00039 .0009 .00039 .0078 .0012 .0208  â€¦           â†’ Worksheet #9 (â€œSmoothingâ€)55Add-delta Smoothingï® every previously unseen n-gram is given a low probabilityï® but there are so many of them that too much probability mass is given to unseen eventsï® instead of adding 1, add some other (smaller) positive value ğ›¿ ï® most widely used value for ğ›¿ = 0.5ï® better than add-one, but stillâ€¦B   N  )w w (w C  )w w (wP n1 2n21AddDï¤ï¤ï€«ï€«ï‚¼ï€½ï‚¼56Factors of Training Corpusï® Size: ï± the more, the betterï± but after a while, not much improvementâ€¦ï® bigrams (characters) after 100â€™s million wordsï® trigrams (characters) after some billions of wordsï® Genre (adaptation):ï± training on cooking recipes and testing on aircraft maintenance manuals57Example: Language Identificationï® hypothesis: texts that resemble each other (same author, same language) share similar character/word sequences  ï± In English character sequence â€œingâ€  is more probable than in French  ï® Training phase: ï± construction of the language model ï± with pre-classified documents (known language/author)ï® Testing phase: ï± apply language model to unknown textAutomatic Language Identificationâ€¦ 58Example: Language Identificationï® bigram of characters ï± characters = 26 letters (case insensitive)ï± possible variations: case sensitivity, punctuation, beginning/end of sentence marker, â€¦591. Train a character-based language model for Italian:2. Train a character-based language model for Spanish:3. Given a unknown sentence â€œche bella cosaâ€  is it in Italian or in Spanish?P(â€œche bella cosaâ€) with the Italian LMP(â€œche bella cosaâ€) with the Spanish LM4. Highest probability  language of sentenceâ†’Example: Language Identification A B C D â€¦ Y Z A 0.0014 0.0014 0.0014 0.0014 â€¦ 0.0014 0.0014 B 0.0014 0.0014 0.0014 0.0014 â€¦ 0.0014 0.0014 C 0.0014 0.0014 0.0014 0.0014 â€¦ 0.0014 0.0014 D 0.0042 0.0014 0.0014 0.0014 â€¦ 0.0014 0.0014 E 0.0097 0.0014 0.0014 0.0014 â€¦ 0.0014 0.0014 â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ 0.0014 Y 0.0014 0.0014 0.0014 0.0014 â€¦ 0.0014 0.0014 Z 0.0014 0.0014 0.0014 0.0014 0.0014 0.0014 0.0014    A B C D â€¦ Y Z A 0.0014 0.0014 0.0014 0.0014 â€¦ 0.0014 0.0014 B 0.0014 0.0014 0.0014 0.0014 â€¦ 0.0014 0.0014 C 0.0014 0.0014 0.0014 0.0014 â€¦ 0.0014 0.0014 D 0.0042 0.0014 0.0014 0.0014 â€¦ 0.0014 0.0014 E 0.0097 0.0014 0.0014 0.0014 â€¦ 0.0014 0.0014 â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ 0.0014 Y 0.0014 0.0014 0.0014 0.0014 â€¦ 0.0014 0.0014 Z 0.0014 0.0014 0.0014 0.0014 0.0014 0.0014 0.0014   Googleâ€™s Web 1T 5-gram modelï® 5-gramsï® generated from 1 trillion wordsï® 24 GB compressed ï± Number of tokens: 1,024,908,267,229 ï± Number of sentences: 95,119,665,584 ï± Number of unigrams: 13,588,391 ï± Number of bigrams: 314,843,401 ï± Number of trigrams: 977,069,902 ï± Number of fourgrams: 1,313,818,354 ï± Number of fivegrams: 1,176,470,663ï® See discussion: http://googleresearch.blogspot.com/2006/08/all-our-n-gram-are-belong-to-you.htmlï® See Google Ngram Viewer: http://en.wikipedia.org/wiki/Google_Ngram_Viewer60http://googleresearch.blogspot.com/2006/08/all-our-n-gram-are-belong-to-you.htmlhttp://en.wikipedia.org/wiki/Google_Ngram_ViewerProblem with n-gramsï® Natural language is not linear ....ï® there may be long-distance dependencies.ï± Syntactic dependenciesï® The man next to the large oak tree near â€¦ is tall.ï® The men next to the large oak tree near â€¦ are tall.ï± Semantic dependenciesï® The bird next to the large oak tree near â€¦ flies rapidly.ï® The man next to the large oak tree near â€¦ talks rapidly.ï± World knowledgeï® Michael Jackson, who was featured in ..., is buried in California. ï® Michael BublÃ©, who was featured in ..., is living in California. ï± ...ï® More complex models of language are needed to handle such dependencies.61Menuï® 621. Introduction2. Bag of word model3. n-gram models4. Linguistic features for NLPLinguistic features used for what?63Applicationslinguistic features are hand-engineered and fed to the ML model  64Stages of NLUsource: Luger (2005)67Stages of NLUsource: Luger (2005)Parsing (Syntax): ï® What words are available in a language?  gfiioudd  / tableï® How to arrange words together? the rose is red /  red the rose is68Syntactic Parsing1. Assign the right part of speech (NOUN, VERB, â€¦) to individual words in a text2. Determine how words are put together to form correct sentencesï® The/DET rose/NOUN is/VERB red/ADJ.  ï® Is/VERB red/ADJ the/DET rose/NOUN.69English Parts-of-Speechï® Open (lexical) class words ï± new words can be added easilyï± nouns, main verbs, adjectives, adverbsï± some languages do not have all these categoriesï® Closed (functional) class wordsï± generally function/grammatical words ï± aka stop wordsï± ex. the, in, and, over, beyondâ€¦ï± relatively fixed membershipï± prepositions, determiners, pronouns, conjunctions, â€¦Smurf talk on youtube:https://www.youtube.com/watch?v=7BPx-vl8G00â†’ Worksheet #9 (â€œPart-of-Speech Taggingâ€)https://www.youtube.com/watch?v=7BPx-vl8G00https://www.youtube.com/watch?v=7BPx-vl8G0070Syntaxï®  How parts-of-speech are organised into larger syntactic constituentsï®  Main Constituents:ï± S: sentence              The boy is happy.ï± NP: noun phrase             the little boy from Paris, Sam Smith, I, ï± VP: verb phrase        eat an apple, sing, leave Paris in the night ï± PP: prepositional phrase   in the morning, about my ticketï± AdjP: adjective phrase    really funny, rather clearï± AdvP: adverb phrase        slowly, really slowly71A Parse Treeï® a tree representation of the application of the grammar to a specific sentence.72a CFG consists ofï® set of non-terminal symbols ï± constituents & parts-of-speechï± S, NP, VP, PP, D, N, V, ...ï® set of terminal symbols ï± words & punctuationï± cat, mouse, nurses, eat, ...ï® a non-terminal designated as the starting symbol ï± sentence Sï® a set of re-write rules ï± having a single non-terminal on the LHS and one or more terminal or non-terminal in the RHSï± S --> NP VP ï± NP --> Pro ï± NP --> PN ï± NP --> D N73An Exampleï® Lexicon:N --> flight | trip | breeze | morning // nounV --> is | prefer | like  // verbAdj --> direct | cheapest | first // adjectivePro --> me | I | you | it // pronounPN --> Chicago | United | Los Angeles // proper nounD --> the | a | this // determinerPrep --> from | to | in // prepositionConj --> and | or | but // conjunctionï® Grammar:S --> NP VP // I + prefer UnitedNP --> Pro | PN | D N | D Adj N   // I, Chicago, the morningVP --> V | V NP | V NP PP   // is, prefer + United, PP --> Prep NP   // to Chicago, to I ??â†’ Worksheet #9 (â€œParsingâ€)74Parsingï® parsing: ï± goal: ï® assign syntactic structures to a sentence ï± result: ï® (set of) parse treesï® we need:ï± a grammar: ï® description of the language constructionsï± a parsing strategy: ï® how the syntactic analysis are to be computed 75Parsing Strategiesï® parsing is seen as a search problem through the space of all possible parse treesï± bottom-up (data-directed): words --> grammarï± top-down (goal-directed): grammar --> words ï± breadth-first: compute all paths in parallelï± depth-first: exhaust 1 path before considering anotherï± Heuristic search76Example: John ate the catï® Bottom-up parsing / breadth first1. John ate the cat2. PN ate the cat3. PN V the cat4. PN V ART cat5. PN V ART N6. NP V ART N7. NP V NP8. NP VP9. Sï® Top-down parsing / depth first1. S2. NP VP3. PN VP4. John VP5. John V NP6. John ate NP 7. John ate ART N8. John ate the N9. John ate the cat77Depth-first vs Breadth-first the cat eats the mouse.ï® depth-first: exhaust 1 path before considering anotherï® breadth-first: ï± compute 1 level at a timeï® Heuristic search: ï± e.g. preference to shorter rulesGrammar:(1) S --> NP VP(2) S --> VP(3) S --> Aux NP VP (4) NP --> Det N PP(5) NP -- > Det N(6) PP -- > Prep Nâ€¦Lexicon:(10) Det --> the(11) N --> cat(12) VB --> eatsâ€¦S   NP-VP       VP   Aux-NP-VP  Det-N-PP Det-N â€¦  the  cat Prep-NP   78Summary of Parsing Strategies Depth First Breath First Heuristic Search Top down ïƒ¼ ïƒ¼ ïƒ¼ Bottom up ïƒ¼ ïƒ¼ ïƒ¼   84Stages of NLUsource: Luger (2005)Semantic interpretation: ï® Lexical Semantics : What is the meaning/semantic relations between individual words? Chair:  person?  Furniture?ï® Compositional Semantics: What is the meaning of phrases and sentences?The chairâ€™s leg is broken85Semantic Interpretationï® Map sentences to some representation of its meaningï± e.g., logics, knowledge graph, embeddingâ€¦1. Lexical Semantics ï® i.e., Meaning of individual words2. Compositional Semanticsï® i.e., Meaning of combination of words86Lexical Semanticsï® ie. The meaning of individual wordsï± A word may denote different things (ex. chair)ï± The meaning/sense of words is not clear-cutï± E.g. Overlapping of word senses across languageslegpatteÃ©tapejambe piedanimaljourneyhuman chair87Word Sense Disambiguation (WSD)ï® Determining which sense of a word is used in a specific sentenceï± I went to the bank of Montreal and deposited 50$.ï± I went to the bank of the river and dangled my feet.88ï® WSD can be viewed as typical classification problemï± use machine learning techniques (ex. NaÃ¯ve Bayes classifier, decision tree) to train a systemï± that learns a classifier (a function f) to assign to unseen examples one of a fixed number of senses (categories)ï® Input: ï± Target word: The word to be disambiguated ï± Features? ï® Output: ï± Most likely sense of the wordWSD as a Classification Problem89Features for WSDï®  intuition: ï± sense of a word depends on the sense of surrounding wordsï® ex: bass = fish, musical instrument, ...ï® So use a window of words around the target word as featuresSurrounding words Most probable sense â€¦riverâ€¦ fish â€¦violinâ€¦ instrument â€¦salmonâ€¦ fish â€¦playâ€¦ instrument â€¦playerâ€¦ instrument â€¦stripedâ€¦ fish   90Features for WSDï® Take a window of n words around the target wordï® Encode information about the words around the target wordï± An electric guitar and bass player stand off to one side, not really part of the scene, just as a sort of nod to gringo expectations perhaps.91NaÃ¯ve Bayes WSDï® Goal: choose the most probable sense s* for a word given a vector V of surrounding wordsï® Feature vector V contains: ï± Features: words [fishing, big, sound, player, fly, rod, â€¦]ï± Value: frequency of these words in a window before & after the target word [0, 0, 0, 2, 1, 0, â€¦]ï® Bayes decision rule: ï± s* = argmaxsk P(sk|V) ï± where:ï® S is the set of possible senses for the target wordï® sk is a sense in Sï® V is the feature vector92ï® Training a NaÃ¯ve Bayes classifier = estimating P(vj|sk) and P(sk) from a sense-tagged training corpus= finding the most likely sense kNaÃ¯ve Bayes WSDNumber of occurrences of feature j over the total number of features appearing in windows of SkNumber of  occurrences of sense k over number of all occurrences of ambiguous wordïƒ·ïƒ·ïƒ·ïƒ·ïƒ¸ïƒ¶ïƒ§ïƒ§ïƒ§ïƒ§ïƒ¨ïƒ¦ï€½ïƒ¥ï€«ï€½n1jkjk  s)s|P(v log  )P(s logargmaxs*k  )s,count(v )s,count(v  )s|P(vtktkjkjïƒ¥ï€½ )count(word )count(s  )P(s kk ï€½93Exampleï® Training corpus (context window = Â±3 words):â€¦Today the World Bank/BANK1 and partners are calling for greater reliefâ€¦â€¦Welcome to the Bank/BANK1 of America the nation's leading financial institutionâ€¦ â€¦Welcome to America's Job Bank/BANK1 Visit our site andâ€¦â€¦Web site of the European Central Bank/BANK1 located in Frankfurtâ€¦â€¦The Asian Development Bank/BANK1 ADB a multilateral development financeâ€¦â€¦lounging against verdant banks/BANK2 carving out the...â€¦for swimming, had warned her off the banks/BANK2 of the Potomac. Nobody...ï® Training:ï± P(the|BANK1) = 5/30 P(the|BANK2) = 3/12ï± P(world|BANK1) = 1/30 P(world|BANK2) = 0/12ï± P(and|BANK1) = 1/30 P(and|BANK2) = 0/12ï± â€¦ â€¦ ï± P(off|BANK1) = 0/30 P(off|BANK2) = 1/12ï± P(Potomac|BANK1) = 0/30 P(Potomac|BANK2) = 1/12ï± P(BANK1) = 5/7 P(BANK2) = 2/7 ï® Disambiguation: â€œI lost my left shoe on the banks of the river Nile.â€ï± Score(BANK1)=log(5/7) + log(P(shoe|BANK1))+log(P(on|BANK1))+log(P(the|BANK1)) â€¦ï± Score(BANK2)=log(2/7) + log(P(shoe|BANK2)+log(P(on|BANK2))+log(P(the|BANK2)) â€¦BANK1 BANK294Example (with add 0.5 smoothing)ï® Training corpus (context window = Â±3 words):â€¦Today the World Bank/BANK1 and partners are calling for greater reliefâ€¦â€¦Welcome to the Bank/BANK1 of America the nation's leading financial institutionâ€¦ â€¦Welcome to America's Job Bank/BANK1 Visit our site andâ€¦â€¦Web site of the European Central Bank/BANK1 located in Frankfurtâ€¦â€¦The Asian Development Bank/BANK1 ADB a multilateral development financeâ€¦â€¦lounging against verdant banks/BANK2 carving out the...â€¦for swimming, had warned her off the banks/BANK2 of the Potomac. Nobody...ï® Assume V = 50ï® Training:ï± P(the|BANK1) = (5+.5) / (30+.5V) P(the|BANK2) = (3+.5) / (12 + .5V)ï± P(world|BANK1) = (1+.5) / 55 P(world|BANK2) = (0+.5) / 37ï± P(and|BANK1) = (1+.5) / 55  P(and|BANK2) = (0+.5) / 37ï± â€¦ï± P(off|BANK1) = (0+.5) / 55 P(off|BANK2) = (1+.5) / 37ï± P(Potomac|BANK1) = (0+.5) / 55 P(Potomac|BANK2) = (1+.5) / 37ï± P(BANK1) = 5/7 P(BANK2) = 2/7ï® Disambiguation: â€œI lost my left shoe on the banks of the river Nile.â€ï± Score(BANK1)=log(5/7) + log(P(shoe|BANK1))+log(P(on|BANK1))+log(P(the|BANK1)) â€¦ï± Score(BANK2)=log(2/7) + log(P(shoe|BANK2))+log(P(on|BANK2))+log(P(the|BANK2)) â€¦â†’ Worksheet #9 (â€œWord Sense Disambiguationâ€)104Stages of NLUsource: Luger (2005)ï® Discourse AnalysisHow to relate the meaning of sentences to surrounding sentences?I have to go to the store.  I need butter.I have to go to the university.  I need butter.ï® PragmaticsHow people use language in a social environment?Do you have a child?    Do you have a quarter?ï® World Knowledge How knowledge about the world (history, facts, â€¦) modifies our understanding of text?Bill Gates passed away last night.105Using World Knowledgeï® Using our general knowledge of the world to interpret a sentence/discourseï® E.g.: The trophy would not fit in the brown suitcase because ...     ... it was too big.     ... it was too small.The professor sent the student to see the principal becauseâ€¦â€¦he wanted to see him.â€¦he was throwing paper balls in class.â€¦he could not take it anymore.ï± Ex: Silence of the lambsâ€¦Current Research area: see Winograd Schema Challengehttps://www.youtube.com/watch?v=sbJ89LFheTshttps://en.wikipedia.org/wiki/Winograd_Schema_Challengehttps://en.wikipedia.org/wiki/Winograd_Schema_Challenge106Summary of NLUsource: Luger (2005)Discourse AnalysisPragmaticsWorld Knowledge Lexical SemanticsCompositional SemanticsSyntactic ParsingRemember these slides?108(next lecture!)