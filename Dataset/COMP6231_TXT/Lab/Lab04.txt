COMP6721 Applied Artificial Intelligence (Fall 2023)Lab Exercise #4: Naïve Bayes ClassificationSolutionsQuestion 1 Assume Jim is foraging for wild mushrooms and decides to use the mobile AIapp “MushDoom” to help identify his findings. After scanning a mushroom, theapp categorizes them as Premium Grade, Standard Grade, or Caution Advised.The app has the following accuracy rates based on its database and algorithms:25% of the mushrooms it scans are labeled as Premium Grade, 50% are labeledas Standard Grade, and 25% are labeled as Caution Advised.However, based on historical data and user feedback, mushrooms labeled asPremium Grade by the app have a 5% chance of being poisonous, a mush-room labeled as Standard Grade has a 15% chance of being poisonous, and amushroom labeled as Caution Advised has a 25% chance.If Jim consumes a mushroom identified by the app and falls ill from poisoning:(a) What is the probability that the app had labeled the mushroom asPremium Grade?Given:P (premium) = 0.25P (standard) = 0.5P (caution) = 0.25P (poisonous|premium) = 0.05P (poisonous|standard) = 0.15P (poisonous|caution) = 0.25Ci ∈ {premium, standard, caution}P (poisonous) =∑P (poisonous|Ci) ∗ P (Ci)= 0.05 ∗ 0.25 + 0.15 ∗ 0.5 + 0.25 ∗ 0.25= 0.15P (premium|poisonous) = P (poisonous|premium) ∗ P (premium)P (poisonous)= 0.05 ∗ 0.250.15= 0.083= 8.3%1(b) What is the probability that the app had labeled the mushroom asStandard Grade?P (standard|poisonous) = P (poisonous|standard) ∗ P (standard)P (poisonous)= 0.15 ∗ 0.50.15= 0.5= 50%(c) What is the probability that the app had labeled the mushroom asCaution Advised?P (caution|poisonous) = P (poisonous|caution) ∗ P (caution)P (poisonous)= 0.25 ∗ 0.250.15= 0.417= 41.67%2Question 2 Assume that Cecilia receives many e-mails from her home town in Klinga,where people speak Klinish. If you do not know Klinish, don’t worry. It is asimple language made up of only 1,000 words that all start with the letter “k”.A Klinish document may also contain words that do not start with “k”, butthese are considered out-of-vocabulary words (like a proper name, for example).Jack is trying to help Cecilia sort her Inbox into 3 mail folders (Personal, Workand Promotion). However, Jack does not speak Klinish, so all he has to workfrom are old e-mails that Cecilia has already sorted into the right folders. Thetable below shows a sample of the data that Jack has gathered from Cecilia’sprevious e-mails. The table indicates the frequency of each Klinish word in eachfolder (to be complete, the table should contain 1,000 rows, corresponding toeach word in Klinish). For example, the word kiki appeared 30 times in e-mailslabelled Personal, 50 times in e-mails about Work, and 9 times in Promotione-mails.FolderPersonal Work PromotionWordkami 45 12 17kawa 78 1 67keke 0 5 80kiki 30 50 9koko 6 10 10kotuku 5 27 20koula 17 56 3...Total Nb of Words 20,000 25,000 17,000The table above corresponds to data collected from 50 e-mails labeled Per-sonal, 65 e-mails labeled Work and 45 e-mails labeled Promotion.Based on the data above, Jack is trying to classify the following two e-mails(note that upper and lower cases should not be distinguished):Email 1: Koko kami kawa koula kekeEmail 2: Keke kawa, koko Google koula keke!(a) Use a Naïve Bayes classifier without any smoothing, to classify the twoe-mails above. Use the sum of logs (base 10), and show the score of eachof the 3 classes (Personal, Work and Promotion) and the most likely class.3Priors:P(Personal) = 50 / (50 + 65 + 45)P(Work) = 65 / (50 + 65 + 45)P(Promotion) = 45 / (50 + 65 + 45)Email 1: Koko kami kawa koula kekescore(personal) =log(P (personal)) + log(P (koko|personal)) + log(P (kami|personal))+log(P (kawa|personal)) + log(P (koula|personal)) + log(P (keke|personal))= log(50/160)+log(6/20,000)+log(45/20,000)+log(78/20,000)+log(17/20,000)+log(0/20,000)= −∞score(work) = log(P (work))+ log(P (koko|work))+ log(P (kami|work))+log(P (kawa|work)) + log(P (koula|work)) + log(P (keke|work))= log(65/160)+log(10/25,000)+log(12/25,000)+log(1/25,000)+log(56/25,000)+log(5/25,000)= −17.8546score(promotion) = log(P (promotion)) + log(P (koko|promotion)) +log(P (kami|promotion))+log(P (kawa|promotion))+log(P (koula|promotion))+log(P (keke|promotion))= log(45/160)+log(10/17,000)+log(17/17,000)+log(67/17,000)+log(3/17,000)+log(80/17,000)= −15.2664The highest score is -15.2664 =⇒ the most likely class is promotion.Email 2: Keke kawa, koko Google koula keke!Note:- ignore the word Google- keke counts twicescore(personal) = log(P (personal))+log(P (keke|personal))+log(P (kawa|personal))+log(P (koko|personal)) + log(P (koula|personal)) + log(P (keke|personal))= log(50/160)+log(0/20,000)+log(78/20,000)+log(6/20,000)+log(17/20,000)+log(0/20,000)= −∞score(work) = log(65/160)+log(5/25,000)+log(1/25,000)+log(10/25,000)+log(56/25,000) + log(5/25,000)4= −18.2348score(promotion) = log(45/160)+log(80/17,000)+log(67/17,000)+log(10/17,000)+log(3/17,000) + log(80/17,000)= −14.5938The highest score is -14.5938 =⇒ the most likely class is promotion.(b) Do the same as part A above, but this time use “add 0.5 smoothing” (i.e.,instead of adding the value 1 to each word frequency, add 12 to each wordfrequency). Adjust the smoothing formula accordingly, and show all yourwork. Again, use the sum of logs (base 10), and show the score of each ofthe 3 classes and the most likely class.Solution:FolderPersonal Work PromotionWordkami 45.5 12.5 17.5kawa 78.5 1.5 67.5keke 0.5 5.5 80.5kiki 30.5 50.5 9.5koko 6.5 10.5 10.5kotuku 5.5 27.5 20.5koula 17.5 56.5 3.5...Total 20,000 25,000 17,000Nb of +0.5 x 1,000 +0.5 x 1,000 +0.5 x 1,000Words = 20,500 = 25,500 = 17,500Email 1: Koko kami kawa koula kekescore(personal) = log(P (personal))+log(P (koko|personal))+log(P (kami|personal))+log(P (kawa|personal)) + log(P (koula|personal)) + log(P (keke|personal))= log(50/160) + log(6.5/20, 500) + log(45.5/20, 500) + log(78.5/20, 500) +log(17.5/20, 500) + log(0.5/20, 500)= −16.7561score(work) = log(P (work))+ log(P (koko|work))+ log(P (kami|work))+log(P (kawa|work)) + log(P (koula|work)) + log(P (keke|work))= log(65/160) + log(10.5/25, 500) + log(12.5/25, 500) + log(1.5/25, 500) +log(56.5/25, 500) + log(5.5/25, 500)= −17.63735score(promotion) = log(P (promotion)) + log(P (koko|promotion)) +log(P (kami|promotion))+log(P (kawa|promotion))+log(P (koula|promotion))+log(P (keke|promotion))= log(45/160) + log(10.5/17, 500) + log(17.5/17, 500) + log(67.5/17, 500) +log(3.5/17, 500) + log(80.5/17, 500)= −15.2227The highest score is -15.2227 =⇒ the most likely class is promotion.FolderPersonal Work PromotionWordkami 45.5 12.5 17.5kawa 78.5 1.5 67.5keke 0.5 5.5 80.5kiki 30.5 50.5 9.5koko 6.5 10.5 10.5kotuku 5.5 27.5 20.5koula 17.5 56.5 3.5...Total Nb of Words 20,500 25,500 17,500Email 2: Keke kawa, koko Google koula keke!Note:- ignore the word Google- keke counts twicescore(Personal) = log(P (personal))+log(P (keke|personal))+log(P (kawa|personal))+log(P (koko|personal)) + log(P (koula|personal)) + log(P (keke|personal))= log(50/160) + log(0.5/20,500) + log(78.5/20,500) + log(6.5/20,500) +log(17.5/20,500) + log(0.5/20,500)= −18.7152score(work) = log(65/160)+log(5.5/25,500)+log(1.5/25,500)+log(10.5/25,500)+log(56.5/25,500) + log(5.5/25,500)= −17.9939score(promotion) = log(45/160) + log(80.5/17,500) + log(67.5/17,500) +log(10.5/17,500) + log(3.5/17,500) + log(80.5/17,500)= −14.5599The highest score is -14.5599 =⇒ the most likely class is promotion6Question 3 Let’s write a Python program to train and run a model using the MultinomialNaïve Bayes Classifier. You can use the MNB implementation provided byscikit-learn, a popular machine learning library for Python.1Let’s first check if scikit-learn is already installed and if not, install it:• Activate your desired Anaconda environment (use comp6721 for the envi-ronment created in the first lab):conda activate your_environment_name• Check if scikit-learn is already installed:python -c "import sklearn; print(sklearn.__version__)"If this command returns a version number, it means scikit-learn is alreadyinstalled. If it raises an error, continue with the installation steps below.• Install scikit-learn:conda install scikit-learnHere’s how to get started with Naïve Bayes in scikit-learn:import numpy as npfrom sklearn.naive_bayes import MultinomialNBfrom sklearn.feature_extraction.text import CountVectorizerStart by implementing the Email Spam Classifier you’ve worked through onWorksheet #3. Create the training data:corpus = np.array(['cheap meds for sale','click here for the best meds','book your trip','cheap book sale, not meds','here is the book for you'])To transform the text corpus into a feature vector, you can use scikit-learn’sCountVectorizer :2vectorizer = CountVectorizer()X = vectorizer.fit_transform(corpus)You also need the target vector with the labels for the training data (here,SPAM is 0 and HAM is 1):1See https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html2See https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html7https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.htmlhttps://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.htmlhttps://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.htmly = np.array([0,0,0,1,1])Get a classifier using the prior probabilities for each class (0.6 for SPAM, 0.4for HAM):classifier = MultinomialNB(class_prior=[0.6, 0.4])Train a model using your classifier:model = classifier.fit(X, y)Now you can try to apply your model to classify a new email as SPAM orHAM. Here is the example email (’the cheap book’) as a feature vector:new_mail = [[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]](a) Create a complete, working Python program. Print out the intermediatevariables to see the data you are working with. Predict the class for thenew_mail using your model and print it out.Here is a possible solution:import numpy as npfrom sklearn.naive_bayes import MultinomialNBfrom sklearn.feature_extraction.text import CountVectorizercorpus = np.array(['cheap meds for sale','click here for the best meds','book your trip','cheap book sale, not meds','here is the book for you'])# Create training datavectorizer = CountVectorizer()X = vectorizer.fit_transform(corpus)print(vectorizer.get_feature_names()) # Printing the vocabularyprint(X.toarray()) # Printing the token count vectors for the corpus# Create target vectory = np.array([0,0,0,1,1])print("Target vector = ", y)# Create multinomial naive Bayes classifier# with prior probabilities of each classclassifier = MultinomialNB(class_prior=[0.6, 0.4])# Train a model8model = classifier.fit(X, y)# New email: 'the cheap book'new_mail = [[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]]# Predict new observation's classpredict = model.predict(new_mail)print('Predicted class = ', predict)Note that it prints the numerical values for the predicted class, if youwant to use labels, you can add code like the following:labels = ["SPAM", "HAM"]print(f'Predicted class = {predict} ({labels[predict[0]]})')(b) Inspect the scikit-learn documentation to understand how smoothing ishandled for this algorithm.First, remember that smoothing is introduced to handle the problem ofzero probability. In the context of text classification, if a word is notpresent in the training data for a particular class, it will have a zeroprobability, which will affect the posterior probability. To avoid this,smoothing ensures that every word has a non-zero probability. As youcan see from the scikit-learn documentation,class sklearn.naive_bayes.MultinomialNB(alpha=1.0,fit_prior=True, class_prior=None)the parameter alpha controls smoothing and uses 1.0 by default (this iswhat we also did in the lecture and is called Laplace smoothing3). Tryexperimenting with switching it off by setting the value to 0.(c) Change the code to transform the new_mail automatically from a stringinto a feature vector.When using machine learning models, raw data (like a text email) usuallycannot be processed directly. It has to be transformed into a numericalformat that the model can understand. This is what the CountVectorizerdoes, turning text data into a vector of numbers:T = vectorizer.transform(np.array(['the cheap book']))A common mistake when using CountVectorizer (or any similar tool) isto fit a new vectorizer to the test data, which usually leads to inconsis-tent features between the training and test data. It’s important to usethe same CountVectorizer that you used for your training data. Thisensures that the same word-to-index mapping is maintained. That’s whywe use transform instead of fit_transform (look up the difference in thedocumentation). If you were to fit a new vectorizer to the test data, the3See https://en.wikipedia.org/wiki/Additive_smoothing9https://en.wikipedia.org/wiki/Additive_smoothingresulting vectors will not align with what the model expects, leading toerrors or inaccurate predictions.(d) Change the code to automatically compute the prior probabilities usingthe training data. Print out the priors for the model to verify that theyare indeed correct.If you do not explicitly provide the class priors, like we did above, theywill be automatically computed from the training data. So unless youalready know the priors for a specific problem, you would simply createthe classifier with:classifier = MultinomialNB()You can verify them by printing out the model priors with:print("Class priors ln = ", model.class_log_prior_)which will show you the natural logarithm (ln) values of the computed classpriors. The library provides the logarithm of the probabilities to avoidunderflow issues that can occur when multiplying many small probabilitiestogether, which is a common operation in Naïve Bayes. To get the actualprobabilities from the logged values, you can use the exponential function.Here’s how to convert the logged values back to regular probabilities:print("Class priors = ", np.exp(model.class_log_prior_))10