COMP6721 Applied Artificial Intelligence (Fall 2023)Lab Exercise #5: Decision Trees & K-meansSolutionsQuestion 1 Given the training instances below, use information theory to find whether‘Outlook’ or ‘Windy’ is the best feature to decide when to play a game of golf.Outlook Temperature Humidity Windy Play / Don’t Playsunny 85 85 false Don’t Playsunny 80 90 true Don’t Playovercast 83 78 false Playrain 70 96 false Playrain 68 80 false Playrain 65 70 true Don’t Playovercast 64 65 true Playsunny 72 95 false Don’t Playsunny 69 70 false Playrain 75 80 false Playsunny 75 70 true Playovercast 72 90 true Playovercast 81 75 false Playrain 71 80 true Don’t Play1H(Output) = H( 514 ,914)= −( 514 log2514 + 914 log2914)= 0.94H(Output|sunny) = H(35 ,25)= −(35 log235 + 25 log225)= 0.97H(Output|overcast) = H (0, 1) = − (0 log2 0 + 1 log2 1) = 0H(Output|rain) = H(25 ,35)= −(25 log225 + 35 log235)= 0.97H(Output|Outlook) = 514H(Output|sunny) + 414H(Output|overcast) + 514H(Output|rain)H(Output|Outlook) = 5140.97 + 4140 + 5140.97 = 0.69gain(Outlook) = H(Output)−H(Output|Outlook) = 0.94− 0.69 = 0.25H(Output|Windy = true) = H(12 ,12)= 1H(Output|Windy = false) = H(14 ,34)= 0.81H(Output|Windy) = 6141 + 8140.81 = 0.89gain(Windy) = H(Output)−H(Output|Windy) = 0.94− 0.89 = 0.05⇒ ‘Outlook’ is a better feature because it has a bigger information gain.2Question 2 It’s time to leave the calculations to your computer: Write a Python programthat uses scikit-learn’s Decision Tree Classifier:1import numpy as npfrom sklearn import treefrom sklearn import preprocessingHere is the training data from the first question:dataset = np.array([['sunny', 85, 85, 0, 'Don\'t Play'],['sunny', 80, 90, 1, 'Don\'t Play'],['overcast', 83, 78, 0, 'Play'],['rain', 70, 96, 0, 'Play'],['rain', 68, 80, 0, 'Play'],['rain', 65, 70, 1, 'Don\'t Play'],['overcast', 64, 65, 1, 'Play'],['sunny', 72, 95, 0, 'Don\'t Play'],['sunny', 69, 70, 0, 'Play'],['rain', 75, 80, 0, 'Play'],['sunny', 75, 70, 1, 'Play'],['overcast', 72, 90,1, 'Play'],['overcast', 81, 75, 0, 'Play'],['rain', 71, 80, 1, 'Don\'t Play'],])Note that we changed True and False into 1 and 0.For our feature vectors, we need the first four columns:X = dataset[:, 0:4]and for the training labels, we use the last column from the dataset:y = dataset[:, 4]However, you will not be able to use the data as-is: All features must benumerical for training the classifier, so you have to transform the strings intonumbers. Fortunately, scikit-learn has a preprocessing class for label encodingthat we can use:2le = preprocessing.LabelEncoder()X[:, 0] = le.fit_transform(X[:, 0])(Note: you will need to transform y as well.)1See https://scikit-learn.org/stable/modules/tree.html#classification2See https://scikit-learn.org/stable/modules/preprocessing_targets.html#preprocessing-targets3https://scikit-learn.org/stable/modules/tree.html#classificationhttps://scikit-learn.org/stable/modules/preprocessing_targets.html#preprocessing-targetsNow you can create a classifier object:dtc = tree.DecisionTreeClassifier(criterion="entropy")Note that we are using the entropy option for building the tree, which is themethod we’ve studied in class and used on the exercise sheet. Train the classifierto build the tree:dtc.fit(X, y)Now you can predict a new value using dtc.predict(test), just like you did forthe Naïve Bayes classifier last week. Note: if you want the string output thatyou transformed above, you can use the inverse_transform(predict) functionof a label encoder to change the predicted result back into a string.Visualizing the decision tree can help you understand how the model makespredictions based on the features. It provides insights into the decision-makingprocess of the classifier. A simple way to print the tree is:tree.plot_tree(dtc)but this can be a bit hard to read; to get a prettier version you can use theGraphviz3 visualization software. Graphviz is a powerful open source softwaretool for creating visual representations of graphs and networks. Here, we’llbe using a Python package called graphviz to interface with Graphviz andgenerate visually informative diagrams, like decision trees, directly from ourPython code. You can call it from Python like this:4# print a nicer tree using graphvizimport graphvizdot_data = tree.export_graphviz(dtc, out_file=None,feature_names=['Outlook', 'Temperature', 'Humidity', 'Windy'],class_names=['Don\'t Play', 'Play'],filled=True, rounded=True)graph = graphviz.Source(dot_data)graph.render("mytree")The result will be stored in a file mytree.pdf and should look like Figure 1.Most of the code is provided above. For transforming the labels, you can usethe same label encoder:y = le.fit_transform(dataset[:, 4])To predict a label for a new feature vector:y_pred = dtc.predict([[2, 81, 95, 1]])print("Predicted output: ", le.inverse_transform(y_pred))3See https://www.graphviz.org/4If it is not yet installed, you can use conda install graphviz python-graphviz to install it (note thatyou need both the Graphviz tool and its Python bindings)4https://www.graphviz.org/Outlook <= 0.5entropy = 0.94samples = 14value = [5, 9]class = Playentropy = 0.0samples = 4value = [0, 4]class = PlayTrueTemperature <= 77.5entropy = 1.0samples = 10value = [5, 5]class = Don't PlayFalseTemperature <= 73.5entropy = 0.954samples = 8value = [3, 5]class = Playentropy = 0.0samples = 2value = [2, 0]class = Don't PlayTemperature <= 70.5entropy = 1.0samples = 6value = [3, 3]class = Don't Playentropy = 0.0samples = 2value = [0, 2]class = PlayTemperature <= 66.5entropy = 0.811samples = 4value = [1, 3]class = Playentropy = 0.0samples = 2value = [2, 0]class = Don't Playentropy = 0.0samples = 1value = [1, 0]class = Don't Playentropy = 0.0samples = 3value = [0, 3]class = PlayFigure 1: Generated Decision Tree using scikit-learn: Note that the string values for Outlookhave been changed into numerical values (‘overcast’= 0, ‘rain’= 1, ‘sunny’ = 2)5To print the tree using the encoded class names:dot_data = tree.export_graphviz(dtc, out_file=None,feature_names=['Outlook', 'Temperature', 'Humidity', 'Windy'],class_names=le.classes_,filled=True, rounded=True)6Question 3 Let’s start working with some “real” data: scikit-learn comes with a number ofexample datasets, including the Iris flower dataset. If you do not know thisdataset, start by reading https://en.wikipedia.org/wiki/Iris_flower_data_set.5Figure 2: An Iris versicolor6(a) Change your program from the previous question to work with this newdataset:from sklearn.datasets import load_irisX, y = load_iris(return_X_y=True)This code imports the Iris dataset, with the feature data stored in X andthe target labels in y. Print out same data samples to understand thisdataset with:print(iris.data[:5]) # Display the first five rows of the datasetprint(iris.feature_names) # Display the names of the featuresprint(iris.target_names) # Display the target labelsTrain a decision tree classifier on this dataset and print it out like you didon the previous question.5Also see https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html for thescikit-learn documentation of this dataset.6Photo taken by Danielle Langlois in July 2005 at the Forillon National Park of Canada, Quebec, Canada.Licensed under the Creative Commons Attribution-Share Alike 3.0 Unported license. https://commons.wikimedia.org/wiki/File:Iris_versicolor_3.jpg7https://en.wikipedia.org/wiki/Iris_flower_data_sethttps://en.wikipedia.org/wiki/Iris_flower_data_sethttps://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.htmlhttps://commons.wikimedia.org/wiki/File:Iris_versicolor_3.jpghttps://commons.wikimedia.org/wiki/File:Iris_versicolor_3.jpgHere is a minimal solution:from sklearn import treefrom sklearn.datasets import load_iris# load the Iris datasetiris = load_iris()X, y = iris.data, iris.target# create and print the decision treedtc = tree.DecisionTreeClassifier(criterion="entropy")dtc.fit(X, y)tree.plot_tree(dtc)For a nicer tree plot, again use graphviz:# print a nicer tree using graphvizimport graphvizdot_data = tree.export_graphviz(dtc, out_file=None,feature_names=iris.feature_names,class_names=iris.target_names,filled=True, rounded=True,special_characters=True)graph = graphviz.Source(dot_data)graph.render("iristree")(b) Now you have to evaluate the performance of your classifier. Use scikit-learn’s train_test_split helper function7 to split the Iris dataset into atraining and testing subset, as discussed in the lecture. This allows youto assess how well your classifier generalizes to unseen data.To create an 80%/20% split of the training/testing data, use:from sklearn.model_selection import train_test_splitX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)The random_state parameter ensures consistency in the results every timethe code is run, enabling a reproducible split between the training andtest datasets.After splitting the dataset, apply your decision tree classifier to the testingset to make predictions.y_pred = dtc.predict(X_test)Now you can run the evaluation by computing the Precision, Recall, F1-measure, and Accuracy of your classifier, by using the tools in scikit-learn.87See https://scikit-learn.org/stable/modules/cross_validation.html8See https://scikit-learn.org/stable/modules/model_evaluation.html8https://scikit-learn.org/stable/modules/cross_validation.htmlhttps://scikit-learn.org/stable/modules/model_evaluation.htmlMake sure to print the values of these metrics for your decision tree classi-fier’s performance on the testing set. This evaluation will help you under-stand how well your classifier performs in terms of its ability to correctlyclassify different Iris flower species.You can now generate the individual metrics, e.g., for precision, you canuse:from sklearn.metrics import precision_scoreprecision = precision_score(y_test, y_pred, average='weighted')print("Precision: ", precision)You could do the same for the other metrics. However, scikit-learn alsohas a convenient helper function to produce a report for all the metrics,the classification_report:9from sklearn.metrics import classification_reportprint(classification_report(y_test, y_pred))Which should result in a report like this:precision recall f1-score support0 1.00 1.00 1.00 111 1.00 1.00 1.00 132 1.00 1.00 1.00 6accuracy 1.00 30macro avg 1.00 1.00 1.00 30weighted avg 1.00 1.00 1.00 30As you can see, the data was easy to learn and your classifier has a perfectscore. Try decreasing the training data to just 50% of the dataset to seesome errors.Note: Since we have more than two classes, the above metrics are anaverage of the values for each individual class. You can see that we setthis explicitly when computing the precision score separately.(c) Finally, compute and print out the confusion matrix.10This is easy using the helper function confusion_matrix:from sklearn.metrics import confusion_matrixprint("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))which results in:9See https://scikit-learn.org/stable/modules/model_evaluation.html#classification-report10See https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html9https://scikit-learn.org/stable/modules/model_evaluation.html#classification-reporthttps://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.htmlConfusion Matrix:[[11 0 0][ 0 13 0][ 0 0 6]]Remember from the lecture that the confusion matrix provides a visualrepresentation of our classifier’s performance; in it, each row representsthe true class while each column indicates the predicted class. You cansee that here, the classifier did not misclassify any of the samples in thetest dataset.10Question 4 Consider the following data set with two attributes for each of seven individuals.a1 a2Data1 1.0 1.0Data2 1.5 2.0Data3 3.0 4.0Data4 5.0 7.0Data5 3.5 5.0Data6 4.5 5.0Data7 3.5 4.5(a) The data set is to be grouped into two clusters. Initialize the clusters usingData1 and Data4 as initial centroids to the two clusters. That is, allocatethe remaining individual to one of the two clusters using the Euclideandistance.Individuals CentroidGroup 1 1 (1.0, 1.0)Group 2 4 (5.0, 7.0)For Data2:Distance to Centroid 1:√(1.5− 1.0)2 + (2.0− 1.0)2 ≈ 1.1Distance to Centroid 2:√(1.5− 5.0)2 + (2.0− 7.0)2 ≈ 6.1Distance to Centroid 1 Distance to Centroid 2Data1 0.0 7.2Data2 1.1 6.1Data3 3.6 3.6Data4 7.2 0.0Data5 4.7 2.5Data6 5.3 2.1Data7 4.3 2.9(b) Recalculate the centroids based on the current partition, reassign the in-dividuals based on the new centroids. Which individuals (if any) changedclusters as a result?For Group 1:1.0 + 1.5 + 3.03 = 1.83111.0 + 2.0 + 4.03 ≈ 2.33Individuals CentroidGroup 1 1, 2, 3 (1.83, 2.33)Group 2 4, 5, 6, 7 (4.125, 5.375)Distance to Centroid 1 Distance to Centroid 2Data1 1.6 5.4Data2 0.5 4.3Data3 2.0 1.8Data4 5.6 1.9Data5 3.1 0.7Data6 3.8 0.5Data7 2.7 1.1Data3 changed from Group 1 to Group 2. All the other individuals remainedin the same cluster.12Question 5 scikit-learn also has an implementation for K-means.11(a) Complete clustering the data from the previous question using scikit-learn’sK-means implementation:dataset = np.array([[1.0, 1.0],[1.5, 2.0],[3.0, 4.0],[5.0, 7.0],[3.5, 5.0],[4.5, 5.0],[3.5, 4.5]])Note that with K-means, you do not need any labels, since we are doingunsupervised learning:from sklearn.cluster import KMeanskmeans = KMeans(n_clusters=2)kmeans.fit(dataset)To print the results, you can use:print(kmeans.labels_)This will show you for each element in the dataset which cluster it belongsto.You should get an output like this:[0 0 1 1 1 1 1](b) Now cluster the Iris flower dataset: How well do the clusters representthe three species of Iris?To answer this question, we’ll use Matplotlib,12 a comprehensive libraryfor creating static, animated, and interactive visualizations in Python.Originally developed by John D. Hunter in 2003, it has since become oneof the most widely used data visualization tools in the Python ecosystem.You can use it to create a 3D-plot of your clusters as shown below:# Plot the results using matplotlibimport matplotlib.pyplot as pltfig = plt.figure(figsize=(10, 8))plot = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)plot.set_xlabel('Petal width')plot.set_ylabel('Sepal length')plot.set_zlabel('Petal length')plot.scatter(X[:, 3], X[:, 0], X[:, 2], c=kmeans.labels_, edgecolor='k')11See https://scikit-learn.org/stable/modules/clustering.html#k-means12See https://matplotlib.org/13https://scikit-learn.org/stable/modules/clustering.html#k-meanshttps://matplotlib.org/The result should look similar to Figure 3. The 3D plot visualizes the clus-tering based on three features: petal width, sepal length, and petal length.This visual representation aids in discerning how well the algorithm hasclustered the data points based on these features.Here is a minimal, complete program to cluster the Iris flower dataset:from sklearn.datasets import load_irisfrom sklearn.cluster import KMeans# load the Iris datasetiris = load_iris()X = iris.data# use k-means to cluster the datasetkmeans = KMeans(n_clusters=3)kmeans.fit(X)Plotting the results using Matplotlib works just as described above.Figure 3: Plotting the results of clustering with K-means on the Iris dataset with Matplotlib14