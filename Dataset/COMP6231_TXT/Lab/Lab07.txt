COMP6721 Applied Artificial Intelligence (Fall 2023)Lab Exercise #07: Introduction to Deep LearningSolutionsPyTorch is a deep learning research platform designed for maximum flexibility and speed.1While scikit-learn offers user-friendly tools for a wide range of machine learning algorithms,focusing mainly on traditional methods, PyTorch caters specifically to deep learning. Itprovides a dynamic environment that allows for intricate model designs and optimizations.To gain a basic understanding of how to implement an Artificial Neural Network using thePyTorch library, in the subsequent questions, you will implement both a simple MLP and aconvolutional neural network for a specific image classification task.Installation. To set up the necessary environment using conda, follow the steps below:1. First, create a new conda environment. This helps in maintaining a clean workspaceand avoids conflicts with other packages. Open your terminal or Anaconda promptand run:conda create --name pytorch_env python=3.82. Activate the environment:conda activate pytorch_env3. Install PyTorch and torchvision using the official channel:2conda install pytorch torchvision -c pytorch4. Lastly, ensure you have other required libraries such as matplotlib for visualization, ifnot included in your current environment:conda install matplotlibWith these steps completed, you should have a working conda environment namedpytorch_env with all the necessary libraries installed.1See https://pytorch.org/docs/stable/index.html2See https://pytorch.org/get-started/locally/ for all options1https://pytorch.org/docs/stable/index.htmlhttps://pytorch.org/get-started/locally/Question 1 Let’s use PyTorch to implement a multi-layer perceptron for classifying theCIFAR10 dataset (see Figure 1).3 The torchvision package4 provides dataloaders for common datasets such as Imagenet, CIFAR10, MNIST, etc.Figure 1: Some example images from the CIFAR-10 datasetFirst, utilize the provided code block below, which contains essential Pythonimports and the cifar_loader function, a utility to load the CIFAR-10 dataset.Each CIFAR-10 image is a 32 × 32 RGB image, giving an input size of 3 ×32 × 32 = 3072. The cifar_loader provides train and test data loaders thatcan be used as iterators. To retrieve the data, standard Python iterators likeenumerate can be employed. The training dataset uses data augmentationtechniques, specifically RandomHorizontalFlip and RandomCrop, to artificiallyexpand the dataset.5 Such augmentations introduce variability, making themodel more robust and less prone to overfitting on the training data. Aftersetting the hyper-parameters, where hidden_size specifies the hidden dimen-sion and output_size represents the output dimension, the dataset is loaded.3For details on CIFAR10, see https://en.wikipedia.org/wiki/CIFAR-104https://pytorch.org/docs/stable/torchvision/index.html5The RandomHorizontalFlip augmentation mirrors images, simulating the appearance of objects fromdifferent orientations. The RandomCrop augmentation helps in focusing on various parts of an image, teachingthe model to recognize features irrespective of their position.2https://en.wikipedia.org/wiki/CIFAR-10https://pytorch.org/docs/stable/torchvision/index.htmlimport torchimport torch.nn as nnimport torch.nn.functional as Fimport torch.utils.data as tdimport torchvision.transforms as transformsimport torchvision.datasets as datasets# Function to load CIFAR10 datasetdef cifar_loader(batch_size, shuffle_test=False):# Normalization values for CIFAR10 datasetnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.225, 0.225, 0.225])# Loading training dataset with data augmentation techniquestrain_dataset = datasets.CIFAR10('./data', train=True, download=True,transform=transforms.Compose([transforms.RandomHorizontalFlip(),transforms.RandomCrop(32, 4),transforms.ToTensor(),normalize]))# Loading test datasettest_dataset = datasets.CIFAR10('./data', train=False,transform=transforms.Compose([transforms.ToTensor(),normalize]))# Creating data loaders for training and testingtrain_loader = td.DataLoader(train_dataset, batch_size=batch_size,shuffle=True, pin_memory=True)test_loader = td.DataLoader(test_dataset, batch_size=batch_size,shuffle=shuffle_test, pin_memory=True)return train_loader, test_loader# Hyperparameters and settingsbatch_size = 64test_batch_size = 64input_size = 3 * 32 * 32 # 3 channels, 32x32 image sizehidden_size = 50 # Number of hidden unitsoutput_size = 10 # Number of output classes (CIFAR-10 has 10 classes)num_epochs = 103train_loader, _ = cifar_loader(batch_size)_, test_loader = cifar_loader(test_batch_size)(a) This is the stage where you’ll define the model. You’ve previously workedwith scikit-learn’s MLP, so think of this step as specifying the architectureof the neural network. In PyTorch, the preferred method to create a neuralnetwork is to define a class that inherits from the nn.Module6 superclass.The nn.Module class in PyTorch acts as a blueprint that offers functionali-ties essential for building a variety of deep learning models.For this exercise, you’ll be defining the MultiLayerFCNet class, representinga four-layer, fully connected network:model = MultiLayerFCNet(input_size, hidden_size, output_size)This will include an input layer, two hidden layers, and an output layer.Use the hyper-parameter hidden_size to specify the number of neuronsin each hidden layer. As for the activation functions, employ the ReLU(Rectified Linear Unit) activation for the hidden layers. For the outputlayer, leverage the log_softmax function, which is especially suitable formulti-class classification tasks. The log_softmax function provides thelogarithm of the softmax values, which, when combined with the negativelog likelihood loss, can be used to train models efficiently for classificationtasks in PyTorch.Here’s a possible solution:class MultiLayerFCNet(nn.Module):def __init__(self, D_in, H, D_out):# Initialize the parent class, nn.Modulesuper(MultiLayerFCNet, self).__init__()# Define the input layer (from input dimension to hidden dimension)self.linear1 = torch.nn.Linear(D_in, H)# Define two hidden layersself.linear2 = torch.nn.Linear(H, H)self.linear3 = torch.nn.Linear(H, H)# Define the output layer (from hidden dimension to output dimension)self.linear4 = torch.nn.Linear(H, D_out)def forward(self, x):# Pass input through the input layer and apply ReLU activationx = F.relu(self.linear1(x))# Pass through the first hidden layer and apply ReLU activation6https://pytorch.org/docs/stable/generated/torch.nn.Module.html4https://pytorch.org/docs/stable/generated/torch.nn.Module.htmlx = F.relu(self.linear2(x))# Pass through the second hidden layer and apply ReLU activationx = F.relu(self.linear3(x))# Pass through the output layerx = self.linear4(x)# Apply log_softmax to get log probabilities for multi-class classificationreturn F.log_softmax(x, dim=1)The forward function overrides the base forward function in nn.Moduleandis required for the nn.Module functionality to work correctly. It definesthe computational steps for data as it moves through the network.(b) Now use PyTorch’s CrossEntropyLoss7 to construct the loss function. Thisloss is particularly suited for classification problems where the task is topredict one out of multiple classes.Define an optimizer using torch.optim8. The optimizer is responsible forupdating the weights of our neural network based on the gradients com-puted during back-propagation. Specifically, we will be using the Stochas-tic Gradient Descent (SGD) optimizer. The learning rate (lr) controlsthe step size during optimization, and momentum helps accelerate gradientsvectors in the right directions, leading to faster converging.The first argument passed to the optimizer function contains the parame-ters we want the optimizer to train. By passing model.parameters() to thefunction, PyTorch effortlessly tracks all the parameters within the modelthat require training:criterion = torch.nn.CrossEntropyLoss()optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)Next, loop over the number of epochs. Within this loop, pass the modeloutputs and the true labels to the CrossEntropyLoss function, defined ascriterion. Back-propagation is then performed to compute the gradientof the loss with respect to the model parameters. Call backward() on theloss variable to execute this. After calculating the gradients using back-propagation, invoke optimizer.step() to perform the optimizer’s weightupdate step.To train the model, we’ll iterate over the dataset multiple times, which areknown as epochs. For each epoch:i. We iterate through batches of data from the train_loader.ii. Reshape the image data to be suitable for our model.iii. Obtain predictions from the model by passing the reshaped images.7https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html8https://pytorch.org/docs/stable/optim.html5https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.htmlhttps://pytorch.org/docs/stable/optim.htmliv. Compute the loss by comparing the model’s predictions to the truelabels.v. Perform backpropagation to compute gradients.vi. Update the model’s parameters using the optimizer.vii. Track and display the average loss for monitoring purposes.Here’s the code that accomplishes this:# Train the modelfor epoch in range(num_epochs):avg_loss_epoch = 0 # To keep track of average loss for each epochbatch_loss = 0 # Sum of losses for the batches processedtotal_batches = 0 # Total batches processed# Iterate over batches of data from train_loaderfor i, (images, labels) in enumerate(train_loader):# Reshape images to (batch_size, 32*32*3) - for RGB images of size 32x32images = images.reshape(-1, 32 * 32 * 3)# Get model predictions for the current batchoutputs = model(images)# Compute the loss between the predicted outputs and true labelsloss = criterion(outputs, labels)# Clear previous gradientsoptimizer.zero_grad()# Backpropagate to compute gradientsloss.backward()# Update model parametersoptimizer.step()total_batches += 1batch_loss += loss.item()# Compute average loss for the current epochavg_loss_epoch = batch_loss / total_batchesprint('Epoch [{}/{}], Average Loss for epoch[{}] = {:.4f}'.format(epoch + 1, num_epochs, epoch + 1, avg_loss_epoch))(c) Finally, we need to monitor the accuracy on the test set. To determinethe model’s predictions, we can use the torch.max()9 function. This func-tion returns the index of the maximum value in a tensor (an array-likestructure). In the context of classification, this index corresponds to thepredicted class label. When using torch.max(), the first argument shouldbe the output from the model you’re examining, and the second argument9https://pytorch.org/docs/stable/generated/torch.max.html6https://pytorch.org/docs/stable/generated/torch.max.htmlspecifies the dimension over which to find the maximum value. Your taskis to report the accuracy on the test set by comparing the predicted classlabels (obtained using torch.max()) to the actual labels.To evaluate the accuracy of the model on the test set:i. We initialize two counters: correct for the number of correct predic-tions and total for the total number of images.ii. Loop through the test data in test_loader.iii. For each batch of test data, we reshape the images and get predictionsfrom the model.iv. We then use torch.max() to get the index of the maximum value foreach prediction, which represents the predicted class label.v. We compare these predicted labels to the true labels to count howmany were correct.vi. After looping through all test data, we calculate and print the accuracy.Here’s the code that accomplishes this:# Initialize counterscorrect = 0total = 0# Loop through test datafor images, labels in test_loader:# Reshape images to match the input size of the modelimages = images.reshape(-1, 3 * 32 * 32)# Get predictions for the current batch of test dataoutputs_test = model(images)# Get predicted class labels using torch.max()_, predicted = torch.max(outputs_test.data, 1)# Update total number of images processedtotal += labels.size(0)# Update correct counter by comparing predicted labels to true labelscorrect += (predicted == labels).sum().item()# Calculate and print accuracyprint('Accuracy of the network on the 10000 test images: %d %%'% (100 * correct / total))After running the accuracy calculation code, you might get a result likethis:Accuracy of the network on the 10000 test images: 45 %7Is this what you’d expect, given our approach?Considering a dataset with 10 classes, random guessing would yield anaccuracy close to 10%. Our model’s performance of (here) 45% suggestsit has learned patterns from the training data. However, it’s essential torecognize the limitations of our current architecture. Simple feed-forwardnetworks aren’t optimized for image data. In the next question, we’ll ex-plore Convolutional Neural Networks (CNNs) that are specifically tailoredfor image processing tasks, potentially offering significant improvements inaccuracy.(d) Bonus visualization: If you want to print some random images from the testset with their classification, you can add the code below to your program:# Plot some example images with their predicted labelsimport numpy as npimport matplotlib.pyplot as pltdef denormalize(tensor, mean, std):for t, m, s in zip(tensor, mean, std):t.mul_(s).add_(m)return tensordef plot_results(x=2, y=5):# Width per image (inches)width_per_image = 2.4# Get a batch of images and labelsdata_iter = iter(test_loader)images, labels = next(data_iter)# Select x*y random images from the batchindices = np.random.choice(images.size(0), x*y, replace=False)random_images = images[indices]random_labels = labels[indices]# Get predictions for these imagesrandom_images_reshaped = random_images.reshape(-1, 3 * 32 * 32)outputs = model(random_images_reshaped)_, predicted = torch.max(outputs.data, 1)# Fetch class names from CIFAR10classes = test_loader.dataset.classesfig, axes = plt.subplots(x, y, figsize=(y * width_per_image, x *↪→ width_per_image))# Iterate over the random images and# display them along with their predicted labels8for i, ax in enumerate(axes.ravel()):# Denormalize imageimg = denormalize(random_images[i], [0.485, 0.456, 0.406],↪→ [0.225, 0.225, 0.225])img = img.permute(1, 2, 0).numpy() # Convert image from CxHxW to↪→ HxWxC format for plottingtrue_label = classes[random_labels[i]]pred_label = classes[predicted[i]]ax.imshow(img)ax.set_title(f"true='{true_label}', pred='{pred_label}'",↪→ fontsize=8)ax.axis('off')plt.tight_layout()plt.show()# Call with optional x, y valuesplot_results()This will give you an output similar to the one shown in Figure 2.Figure 2: Some classification results on the CIFAR10 data9Question 2 To improve the performance for image classification, we will use PyTorch toimplement more complicated, deep learning networks. In this question, youwill implement a convolutional neural network (CNN) step-by-step to classifythe CIFAR-10 dataset. The CNN architecture that we are going to build canbe seen in the diagram below:(a) First, use the following code block, which provides the Python imports, thecifar_loader to load the dataset, and the hyper-parameters definition:from torch.utils.data import DataLoaderimport torchimport torch.nn as nnimport torchvision.transforms as transformsimport torchvision.datasetsnum_epochs = 4num_classes = 10learning_rate = 0.001transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])trainset = torchvision.datasets.CIFAR10(root='./data', train=True,download=True, transform=transform)train_loader = torch.utils.data.DataLoader(trainset, batch_size=32,shuffle=True, num_workers=2)testset = torchvision.datasets.CIFAR10(root='./data', train=False,download=True, transform=transform)test_loader = torch.utils.data.DataLoader(testset, batch_size=1000,shuffle=False, num_workers=2)classes = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')(b) Now create a class inheriting from the nn.Module to define different layersof the network based on provided network architecture above. The firststep is to use the nn.Sequential module10 to create sequentially ordered10https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html10https://pytorch.org/docs/stable/generated/torch.nn.Sequential.htmllayers in the network. It’s a handy way of creating a convolution + ReLU+ pooling sequence. In each convolution layer, use LeakyRelu for the acti-vation function and BatchNorm2d11 to accelerate the training process.class CNN(nn.Module):def __init__(self):super(CNN, self).__init__()self.conv_layer = nn.Sequential(nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),nn.BatchNorm2d(32),nn.LeakyReLU(inplace=True),nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1),nn.BatchNorm2d(32),nn.LeakyReLU(inplace=True),nn.MaxPool2d(kernel_size=2, stride=2),nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),nn.BatchNorm2d(64),nn.LeakyReLU(inplace=True),nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),nn.BatchNorm2d(64),nn.LeakyReLU(inplace=True),nn.MaxPool2d(kernel_size=2, stride=2),)self.fc_layer = nn.Sequential(nn.Dropout(p=0.1),nn.Linear(8 * 8 * 64, 1000),nn.ReLU(inplace=True),nn.Linear(1000, 512),nn.ReLU(inplace=True),nn.Dropout(p=0.1),nn.Linear(512, 10))def forward(self, x):# conv layersx = self.conv_layer(x)# flattenx = x.view(x.size(0), -1)# fc layer11https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d11https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2dx = self.fc_layer(x)return x(c) Before training the model, you have to first create an instance of theConvolution class you defined in previous part, then define the loss functionand optimizer:model = CNN()criterion = nn.CrossEntropyLoss()optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)The following steps are similar to what you’ve done in previous ques-tions: Loop over the number of epochs and within this loop, pass themodel outputs and true labels to the CrossEntropyLoss function, definedas criterion. Then, perform back-propagation and an optimized train-ing. Call backward() on the loss variable to perform the back-propagation.Now that the gradients have been calculated in the back-propagation, calloptimizer.step() to perform the optimizer training step.total_step = len(train_loader)loss_list = []acc_list = []for epoch in range(num_epochs):for i, (images, labels) in enumerate(train_loader):# Forward passoutputs = model(images)loss = criterion(outputs, labels)loss_list.append(loss.item())# Backprop and optimisationoptimizer.zero_grad()loss.backward()optimizer.step()# Train accuracytotal = labels.size(0)_, predicted = torch.max(outputs.data, 1)correct = (predicted == labels).sum().item()acc_list.append(correct / total)if (i + 1) % 100 == 0:print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'.format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),(correct / total) * 100))12(d) Now keep track of the accuracy on the test set. The predictions of themodel can be determined by using torch.max().12model.eval()with torch.no_grad():correct = 0total = 0for images, labels in test_loader:outputs = model(images)_, predicted = torch.max(outputs.data, 1)total += labels.size(0)correct += (predicted == labels).sum().item()print('Test Accuracy of the model on the 10000 test images: {} %'.format((correct / total) * 100))(e) In PyTorch, the learnable parameters (i.e., weights and biases) of a modelare contained in the model’s parameters. A state_dict is simply a Pythondictionary object that maps each layer to its parameter tensor. Becausestate_dict objects are Python dictionaries, they can be easily saved, up-dated, altered, and restored.13Saving the model’s state_dict with the torch.save() function will giveyou the most flexibility for restoring the model later. To load the models,first initialize the models and optimizers, then load the dictionary locallyusing torch.load().Inspect the PyTorch documentation to understand how you can use thesefunctionalities.To save a model:torch.save(modelA.state_dict(), PATH)To restore a model:modelB = TheModelBClass(*args, **kwargs)modelB.load_state_dict(torch.load(PATH), strict=False)12https://pytorch.org/docs/stable/generated/torch.max.html13https://pytorch.org/tutorials/beginner/saving_loading_models.html13https://pytorch.org/docs/stable/generated/torch.max.htmlhttps://pytorch.org/tutorials/beginner/saving_loading_models.htmlQuestion 3 The goal of skorch14 is to make it possible to use PyTorch with scikit-learn.This is achieved by providing a wrapper around PyTorch that has a scikit-learn interface. Additionally, skorch abstracts away the training loop, a simplenet.fit(X, y) is enough. It also takes advantage of scikit-learn functions, suchas predict.(a) In this section, we will train the same CNN model you developed in theprevious question using skorch with fewer lines of code.Let’s install skorch first:pip install skorchOnce the library is installed, we can import the libraries. The next step isto prepare the dataset before training.import matplotlib.pyplot as pltimport numpy as npimport torchimport torch.nn as nnimport torch.optim as optimimport torchvision.datasetsimport torchvision.transforms as transformsfrom sklearn.metrics import accuracy_scorefrom sklearn.metrics import plot_confusion_matrixfrom skorch import NeuralNetClassifierfrom torch.utils.data import random_splitnum_epochs = 4num_classes = 10learning_rate = 0.001transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])trainset = torchvision.datasets.CIFAR10(root='./data', train=True,download=True, transform=transform)testset = torchvision.datasets.CIFAR10(root='./data', train=False,download=True, transform=transform)m = len(trainset)train_data, val_data = random_split(trainset, [int(m - m * 0.2), int(m * 0.2)])DEVICE = torch.device("cpu")y_train = np.array([y for x, y in iter(train_data)])classes = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')skorch.NeuralNetClassifier15 is a Neural Network class used for classifi-cation tasks. Initialize the NeuralNetClassifier class then train the CNN14https://skorch.readthedocs.io/en/stable/index.html15https://skorch.readthedocs.io/en/stable/classifier.html14https://skorch.readthedocs.io/en/stable/index.htmlhttps://skorch.readthedocs.io/en/stable/classifier.htmlmodel using the method fit. Finally print the accuracy score and confu-sion matrix. Note that CNN is the model you already developed from theprevious question using torch.torch.manual_seed(0)net = NeuralNetClassifier(CNN,max_epochs=1,iterator_train__num_workers=4,iterator_valid__num_workers=4,lr=1e-3,batch_size=64,optimizer=optim.Adam,criterion=nn.CrossEntropyLoss,device=DEVICE)net.fit(train_data, y=y_train)y_pred = net.predict(testset)y_test = np.array([y for x, y in iter(testset)])accuracy_score(y_test, y_pred)plot_confusion_matrix(net, testset, y_test.reshape(-1, 1))plt.show()(b) Now let’s evaluate the score using K-fold cross-validation. Before theevaluation, we need to make the training set sliceable using the classSliceDataset, which wraps a torch dataset to make it work with scikit-learn. Use the function cross_val_score(estimator, X, y=None, cv=None)of the scikit-learn library to evaluate the validation accuracy obtained usingK = 5 folds for K-fold cross-validation.To achieve this, replace the training process from the previous section witha K-fold cross-validation.Here is a possible solution:net.fit(train_data, y=y_train)train_sliceable = SliceDataset(train_data)scores = cross_val_score(net, train_sliceable, y_train, cv=5,scoring="accuracy")15