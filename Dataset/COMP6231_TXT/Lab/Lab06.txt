COMP6721 Applied Artificial Intelligence (Fall 2023)Lab Exercise #6: Artificial Neural NetworksSolutionsQuestion 1 Given the training instances below, use scikit-learn to implement a Perceptronclassifier1 that classifies students into two categories, predicting who will getan ‘A’ this year, based on an input feature vector ~x. Here’s the training dataagain:Feature(x) Output f(x)Student ’A’ last year? Black hair? Works hard? Drinks? ’A’ this year?X1: Richard Yes Yes No Yes NoX2: Alan Yes Yes Yes No YesX3: Alison No No Yes No NoX4: Jeff No Yes No Yes NoX5: Gail Yes No Yes Yes YesX6: Simon No Yes Yes Yes NoUse the following Python imports for the perceptron:import numpy as npfrom sklearn.linear_model import PerceptronAll features must be numerical for training the classifier, so you have to trans-form the ‘Yes’ and ‘No’ feature values to their binary representation:# Dataset with binary representation of the featuresdataset = np.array([[1,1,0,1,0],[1,1,1,0,1],[0,0,1,0,0],[0,1,0,1,0],[1,0,1,1,1],[0,1,1,1,0],])For our feature vectors, we need the first four columns:X = dataset[:, 0:4]and for the training labels, we use the last column from the dataset:1https://scikit-learn.org/stable/modules/linear_model.html#perceptron1https://scikit-learn.org/stable/modules/linear_model.html#perceptrony = dataset[:, 4](a) Now, create a Perceptron classifier (same approach as in the previous labs)and train it.Most of the solution is provided above. Here is the additional code requiredto create a Perceptron classifier and train it using the provided dataset:perceptron_classifier = Perceptron(max_iter=40, eta0=0.1, random_state=1)perceptron_classifier.fit(X,y)The parameters we’re using here are:max_iter The maximum number of passes over the training data (akaepochs). It’s set to 40, meaning the dataset will be passed 40 times tothe Perceptron during training.eta0 This is the learning rate, determining the step size during the weightsupdate in each iteration. A value of 0.1 is chosen, which is a moderatelearning rate.random_state This ensures reproducibility of results. The classifier willproduce the same output for the same input data every time it’s run,aiding in debugging and comparison.Try experimenting with these values, for example, by changing the numberof iterations or learning rate. Make sure you understand the significanceof setting random_state.(b) Let’s examine our trained Perceptron in more detail. You can look at theweights it learned with:print("Weights: ", perceptron_classifier.coef_)And the bias, here called intercept term, with:print("Bias: ", perceptron_classifier.intercept_)The activation function is not directly exposed, but scikit-learn is using thestep activation function. Now check how your Perceptron would classify atraining sample by computing the net activation (input vector × weights+ bias) and applying the step function.You can use the following code to compute the net activation on all trainingdata samples and compare this with your results:net_activation = np.dot(X, perceptron_classifier.coef_.T) +↪→ perceptron_classifier.intercept_print(net_activation)2Remember that the step activation function classifies a sample as 1 if thenet activation is non-negative and 0 otherwise. So, if a net activation isnon-negative, the perceptron’s step function would classify it as 1, andotherwise, it would classify it as 0.(c) Apply the trained model to all training samples and print out the predic-tion.This works just like for the other classifiers we used before:y_pred = perceptron_classifier.predict(X)print(y_pred)This will print the classification results like:[0 1 0 0 1 0]Compare the predicted labels with the actual labels from the dataset. Howmany predictions match the actual labels? What does this say about theperformance of our classifier on the training data?3Question 2 Consider the neural network shown below. It consists of 2 input nodes, 1 hiddennode, and 2 output nodes, with an additional bias at the input layer (attachedto the hidden node) and a bias at the hidden layer (attached to the outputnodes). All nodes in the hidden and output layers use the sigmoid activationfunction (σ).(a) Calculate the output of y1 and y2 if the network is fed ~x = (1, 0) as input.hin = bh + wx1-hx1 + wx2-hx2 = (0.1) + (0.3× 1) + (0.5× 0) = 0.4h = σ(hin) = σ(0.4) = 11 + e−0.4 = 0.599y1,in = by1 + wh-y1h = 0.6 + (0.2× 0.599) = 0.72y1 = σ(0.72) = 11 + e−0.72 = 0.673y2,in = by2 + wh-y2h = 0.9 + (0.2× 0.599) = 1.02y2 = σ(1.22) = 11 + e−1.02 = 0.735As a result, the output is calculated as y = (y1, y2) = (0.673, 0.735).(b) Assume that the expected output for the input ~x = (1, 0) is supposed tobe ~t = (0, 1). Calculate the updated weights after the backpropagation ofthe error for this sample. Assume that the learning rate η = 0.1.δy1 = y1(1− y1)(y1 − t1) = 0.673(1− 0.673)(0.673− 0) = 0.148δy2 = y2(1− y2)(y2 − t2) = 0.735(1− 0.735)(0.735− 1) = −0.0524δh = h(1−h)∑i=1,2wh-yiδyi= 0.599(1−0.599)[0.2×0.148+0.2×(−0.052)] = 0.005∆wx1-h = −ηδhx1 = −0.1× 0.005× 1 = −0.0005∆wx2-h = −ηδhx2 = −0.1× 0.005× 0 = 0∆bh = −ηδh = −0.1× 0.005 = −0.0005∆wh-y1 = −ηδy1h = −0.1× 0.148× 0.599 = −0.0088652∆by1 = −ηδy1 = −0.1× 0.148 = −0.0148∆wh-y2 = −ηδy2h = −0.1× (−0.052)× 0.599 = 0.0031148∆by2 = −ηδy2 = −0.1× (−0.052) = 0.0052wx1-h,new = wx1-h + ∆wx1-h = 0.3 + (−0.0005) = 0.2995wx2-h,new = wx2-h + ∆wx2-h = 0.5 + 0 = 0.5bh,new = bh + ∆bh = 0.1 + (−0.0005) = 0.0995wh-y1,new = wh-y1 + ∆wh-y1 = 0.2 + (−0.0088652) = 0.1911348by1,new = by1 + ∆by1 = 0.6 + (−0.0148) = 0.5852wh-y2,new = wh-y2 + ∆wh-y2 = 0.2 + 0.0031148 = 0.2031148by2,new = by2 + ∆by2 = 0.9 + 0.0052 = 0.90525Question 3 Let’s see how we can build multi-layer neural networks using scikit-learn.2(a) Implement the architecture from the previous question using scikit-learnand use it to learn the XOR function, which is not linearly separable.Use the following Python imports:import numpy as npfrom sklearn.neural_network import MLPClassifierHere is the training data for the XOR function:dataset = np.array([[1,1,0],[0,1,1],[1,0,1],[0,0,0]])For our feature vectors, we need the first two columns:X = dataset[:, 0:2]and for the training labels, we use the last column from the dataset:y = dataset[:, 2]Now you can create a multi-layer Perceptron using scikit-learn’s MLP (multi-layer perceptron) classifier.3 There are a lot of parameters you can chooseto define and customize, here you need to define the hidden_layer_sizes.For this parameter, you pass in a tuple consisting of the number of neu-rons you want at each layer, where the nth entry in the tuple represents thenumber of neurons in the nth layer of the MLP model. You also need toset the activation to ‘logistic’, which is the logistic Sigmoid function. Thebias and weight details are implicitly defined in the function definition.Using the code blocks provided above, you can create the network andtrain it on the XOR dataset with:mlp = MLPClassifier(hidden_layer_sizes=(1,),activation='logistic')mlp.fit(X, y)(b) Now apply the trained model to all training samples and print out itsprediction.y_pred = mlp.predict(X)print(y_pred)As you see, our single hidden layer with a single neuron doesn’t performwell on learning XOR. It’s always a good idea to experiment with differentnetwork configurations. Try to change the number of hidden neurons tofind a solution!2https://scikit-learn.org/stable/modules/neural_networks_supervised.html3https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html6https://scikit-learn.org/stable/modules/neural_networks_supervised.htmlhttps://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.htmlWith a single hidden neuron, it can converge in theory, but it is difficult inpractice, highly depending on initial weights and other hyperparameters.With two neurons in the hidden layer, it’s possible but not guaranteed tofind a solution. The success of training depends on the weight initializationand the optimization algorithm’s ability to find a suitable combination ofweights. Often, it may get stuck in local minima.Using three neurons in the hidden layer increases the representational ca-pacity of the network, making it more likely to converge to a solution forthe XOR problem, Try:mlp = MLPClassifier(hidden_layer_sizes=(3,), activation='logistic',↪→ solver='lbfgs', max_iter=100000, random_state=42)7Question 4 Create a multi-layer Perceptron and use it to classify the MNIST digits dataset,containing scanned images of hand-written numerals:4(a) Load MNIST from scikit-learn’s builtin datasets.5 Like before, use thetrain_test_split6 helper function to split the digits dataset into a train-ing and testing subset. Create a multi-layer Perceptron, like in the pre-vious question and train the model. Pay attention to the required size ofthe input and output layers and experiment with different hidden layerconfigurations.import numpy as npfrom sklearn import datasetsfrom sklearn.neural_network import MLPClassifierfrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplayfrom sklearn.metrics import precision_score, recall_scoreimport matplotlib.pyplot as pltMNIST digits is another built-in dataset in scikit-learn. First load thedataset. Since it contains two-dimensional image data, you need to flattenit, so it can be presented to our neural network as input:digits = datasets.load_digits() # 2D images in feature matrixn_samples = len(digits.images) # number of samplesdata = digits.images.reshape((n_samples, -1)) # flatten 2D images into 1DThe third line above “flattens” the 2D image arrays, so that the resultingdata contains a 1D-vector for each image. Thus, data now contains onerow for each image in the dataset, with one column for each pixel in thoseimages and its value representing a gray scale pixel in the image.Create training and test splits (reserving 30% of the data for testing andusing the rest of it for training):4https://en.wikipedia.org/wiki/MNIST_database5https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html6https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html8https://en.wikipedia.org/wiki/MNIST_databasehttps://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.htmlhttps://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.htmlhttps://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.htmlX_train, X_test, y_train, y_test = train_test_split(data, digits.target, test_size=0.3, shuffle=False)Finally, train a neural network that can actually make predictions with:mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, alpha=1e-4,solver='sgd', verbose='true', random_state=1,learning_rate_init=0.001)mlp.fit(X_train, y_train)(b) Now run an evaluation to compute the performance of your model usingscikit-learn’s7 accuracy score.You can evaluate the model with:y_pred = mlp.predict(X_test)print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))Bonus visualization: If you want to print out some example images fromthe test set with their predicted label, you can use the code below:# Randomly select 10 images and print them with their predicted labelsn, m = 2, 5random_indices = np.random.choice(X_test.shape[0], n*m, replace=False)selected_images = X_test[random_indices]selected_predictions = y_pred[random_indices]# Plot the selected images with their predictions in a 2x5 matrixplt.figure(figsize=(10, 4))for i in range(n):for j in range(m):idx = i*m + jplt.subplot(n, m, idx + 1)plt.imshow(selected_images[idx].reshape((8, 8)), cmap='gray')plt.title(f'Predicted: {selected_predictions[idx]}')plt.axis('off')plt.tight_layout()plt.show()(c) In any classification task, whether binary or multi-class, it’s crucial toassess how well the model is doing. Precision and recall are commonlyused metrics for this purpose. For binary classification, their computationis straightforward. However, when we move to multi-class problems, thelandscape becomes more complex. This is where micro and macro averag-ing come in, and they provide two different perspectives:7https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html9https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.htmlMicro-Averaging: This method gives a global view. It pools togetherthe individual true positives, false negatives, and false positives acrossall classes, effectively treating the multi-class problem as a single bi-nary classification. It provides an overall sense of how the model isperforming, without differentiating between classes.Macro-Averaging: This method breaks down the performance by class.It calculates precision and recall for each class separately and then av-erages them. This means every class, regardless of its size, has an equalsay in the final score. It’s useful for understanding the model’s perfor-mance on individual classes, especially when there are imbalances inclass sizes.Both of these methods are standard in the field of machine learning andnot specific to any particular library, including scikit-learn. They offercomplementary perspectives: while micro-averaging might show how wellthe model performs overall, macro-averaging can highlight if it’s strugglingwith any particular class.Run an evaluation on your results and compute the precision and recallscore with micro and macro averaging, using scikit-learn’s precision_score8and recall_score.9 Make sure you compute these on your test set!pre_macro = precision_score(y_test, y_pred, average='macro')pre_micro = precision_score(y_test, y_pred, average='micro')recall_macro = recall_score(y_test, y_pred, average='macro')recall_micro = recall_score(y_test, y_pred, average='micro')Here, the micro and macro averages are very similar, as the classes in thisdataset are mostly balanced. If one class has significantly fewer samples,macro-averaging will give you a sense of how well the model performs onthat specific class compared to the others.(d) Use the confusion matrix implementation from the scikit-learn package tovisualize your classification performance.The confusion matrix provides a more detailed breakdown of a classifier’sperformance, allowing you to see not just where it got things right, butwhere mistakes are being made. Each row in the matrix represents the trueclasses, while each column represents the predicted classes. It’s a powerfultool to understand misclassifications, especially in multi-class problems.cm = confusion_matrix(y_test, y_pred)ConfusionMatrixDisplay(cm, digits.target_names).plot()plt.show()You should get an output similar to the following:8https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html9https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html10https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.htmlhttps://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.htmlBy examining the heatmap, you can quickly identify which classes themodel is confusing with others. The diagonal elements represent the num-ber of points for which the predicted label is equal to the true label, whileoff-diagonal elements are those that are mislabeled by the classifier.(e) K-fold cross-validation is a way to improve the training process: The dataset is divided into k subsets, and the method is repeated k times. Eachtime, one of the k subsets is used as the test set and the other k−1 subsetsare put together to form a training set. Then the average error across allk trials is computed. The advantage of this method is that it matters lesshow the data gets divided. Every data point gets to be in a test set exactlyonce, and gets to be in a training set k − 1 times. The disadvantage ofthis method is that the training algorithm has to be rerun from scratch ktimes, which means it takes k times as much computation to complete anevaluation.10For this task, don’t use the train_test_split created earlier, instead usethe KFold11 class from the scikit-learn package to divide your dataset intok folds. For each fold, train your MLP model on the training set andevaluate its performance on the test set. Calculate performance metricslike accuracy, precision, and recall for each fold. After all folds have beenprocessed, compute the average performance across all folds.Compare the average performance from cross-validation to the performanceyou achieved with a single train/test split.One option would be to code a loop for the number of loops, performtraining and testing, and then average the results. But scikit-learn has a10https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation11https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html11https://scikit-learn.org/stable/modules/cross_validation.html#cross-validationhttps://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.htmlhelper function that can do this automatically for you, cross_val_score,12here using accuracy:from sklearn import datasetsfrom sklearn.neural_network import MLPClassifierfrom sklearn.model_selection import KFolddigits = datasets.load_digits() # features matrixn_samples = len(digits.images)X = digits.images.reshape((n_samples, -1))y = digits.targetmlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, alpha=1e-4,solver='sgd', verbose='true', random_state=1,learning_rate_init=0.001)# Perform 5-fold cross validation and compute accuracy scoresscores = cross_val_score(mlp, X, y, cv=10, scoring='accuracy')print("Accuracy for each fold:")print(scores)print(f"Average Accuracy: {scores.mean()*100:.2f}%")You can also compute multiple metrics using the cross_validate function:scoring = ['precision_macro', 'recall_macro', 'f1_macro', 'accuracy']scores = cross_validate(mlp, X, y, cv=5, scoring=scoring, return_train_score=False)# Print the results from each foldfor metric, values in scores.items():if 'test_' in metric:print(f"{metric.replace('test_', '')}: {values}")# Print the cross-fold resultsfor key, values in scores.items():print(f"{key}: {values.mean():.4f} (+/- {values.std()*2:.4f})")When examining the cross-validation results, ensure you check for consis-tent performance across folds. Significant variability could hint at under-lying dataset issues or model sensitivities. Also, while the average scoreoffers a broad overview, individual fold results can shed light on model ro-bustness, possibly highlighting susceptibility to certain data splits, eitheroverfitting or underfitting.12https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html12https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.htmlhttps://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html