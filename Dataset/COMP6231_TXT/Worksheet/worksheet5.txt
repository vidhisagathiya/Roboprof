COMP 6721 Applied Artificial Intelligence (Fall 2023)Worksheet #5: Artificial Neural NetworksPerceptron. Calculate your first neuron activation for the Perceptron (only 100 billion−1 more to go!):Activation function:f(~x) ={1, if ~x · ~w ≥ threshold0, otherwise(use a threshold of 0.55):f(~x) =Perceptron Learning. Ok, so for the first training example, the perceptron did not produce the rightoutput. To learn the correct result, it has to adjust the weights: ∆w = η(T −O), where we set η = 0.05(our learning rate). The threshold stays at 0.55. T is the expected output and O the output producedby the perceptron (= f(~x)). (1) Start by computing the output for Richard as before. (2) Check if thecomputed output O is correct or not by comparing it with the expected output T . (3) If the output was notcorrect, compute ∆w. (4) Write down the new weights in the next row (Alan). Remember to only updateweights where the current sample (Richard) had an active connection (i.e., with non-zero input, here “No”= 0, “Yes”= 1). (5) Now repeat the steps, computing the output for Alan using the updated weights:Student w1 w2 w3 w4 f(~x) ok? ∆wRichard 0.2 0.2 0.2 0.2AlanAlisonDelta Rule. In the generalized delta rule for training the perceptron, we add a bias input that is alwaysone and has its own weight (here w3). Weight changes ∆wi now take the input value xi into account. Wewant the perceptron to learn the two-dimensional data shown on the right:Assume we use the sign function and set the learning rate η = 0.2. The weights are initialized randomly asshown in the table. Apply the generalized delta rule for updating the weights:sign(x) ={1 if x ≥ 0−1 otherwise∆wi = η(T −O)xiw′ = w + ∆wData w1 w2 w3 f(~x) ok? ∆w1 ∆w2 ∆w3#1 0.75 0.5 -0.6#2#3#4COMP 6721 Worksheet: Artificial Neural Networks Fall 2023Neural Network for XOR: Backpropagation. To learn a non-linearly separable function like XOR, we’lluse a neural network with a hidden layer. The weights have been initialized randomly (note: here, the biasis set to −1):x1 x2 x1 XOR x21 1 00 0 01 0 10 1 1Oi = sigmoid∑jwjixj=11 + e−(∑j wjixj)Step 1. Compute the output for the three neurons O3, O4 and O5 for the first input (x1 = 1, x2 = 1):O3 = O4 = O5 =Step 2. The next step is to calculate the errorδo ← g′(xo)× Erro = Oo(1−Oo)× (Oo − To)starting from the output neuronO5: δ5 = O5(1−O5)×(O5−T5) =Step 3. Now we calculate the error terms for the hidden layer:δh ← g′(xh)× Errh = Oh(1−Oh)×∑k∈outputsδkwhkFor the two neurons (3), (4) in the hidden layer:• δ3 = O3(1−O3)δ5w35 =• δ4 = O4(1−O4)δ5w45 =Step 4. Now we compute our weight changes, using a constant learning rate η = 0.1:∆wij = −ηδjxi• ∆w14 =• ∆w24 =• ∆w45 =• ∆Θ5 =Step 5. And finally, we update the weights (wij ← wij + ∆wij):• w14 = w14 + ∆w14 =• w24 = w24 + ∆w24 =• w45 = w45 + ∆w45 =• Θ5 = Θ5 + ∆Θ5 =