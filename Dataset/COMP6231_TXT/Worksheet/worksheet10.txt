COMP 6721 Applied Artificial Intelligence (Fall 2023)Worksheet #10: Deep Learning for NLPWord Analogies. Word analogy questions often appear on standardized tests, like the SSAT, to test languageaptitude and reasoning. Here’s a simple one (fill in the blank): Japan is to Sushi what Germany is toHow can we solve this type of question with an AI? Stay tuned for the answer!Word Vectors. Here are three words in one-hot vector representation (three words, so three dimensions):What is the distance between the one-hot word vectors for (cat, dog) and (cat,house):Using the Euclidian distance,d(~p, ~q) =√∑ni=1(pi − qi)2Word2Vec: Training Data. Consider the following sentence: “the cat drinks the milk”. We will use this sentence totrain a CBOW Word2Vec model. Assume that you use a context window of size 2 (1 word before and 1 word afterthe target word), and your vocabulary only contains the words in the sentence above.Using only the sentence above, create the training instances using the CBOW method:Instance Context Word -1 Context Word +1 To Predict123Word2Vec: Input Vectors. Now, (a) encode the vocabulary using one-hot vectors, assuming alphabetical ordering,no stop-word filtering (left) and (b) using these vectors, encode the three training instances above as input vectors forthe network:Word One-Hot VectorcatdrinksmilktheInstance Context Word One-Hot Vector1Context Word -1Context Word +12Context Word -1Context Word +13Context Word -1Context Word +1Word2Vec: Target Vectors. And what is the target one-hot vector for each training instance?Instance To Predict One-Hot Vector123COMP6721 Worksheet: Deep Learning for NLP Fall 2023Word2Vec: Network. Assume that the Word2Vec model is trained with the network depicted below:Assume that you want to produce word embeddings of dimension 2, using the training data you created above:• What is the shape (n×m) of the input layer I?• How many nodes m do we have in the hidden layer H?• And the shape of the output layer O is?• What’s the shape of the weight matrix W from input to hidden layer?• And the shape of the weight matrix W ′ from hidden to output layer is?Note: (1) the Word2Vec network does not use a bias at either layer; (2) we apply one input word vector at a timeand average the output of the hidden layer1Softmax. Compute the softmax activation function σ on the vector v below:σ(z)j =ezj∑Kk=1 ezkv =0.50.90.2 σ(v) = Solving Word Analogies. Ok, now re-write the question from the first task in form of a word vector calculation:1another approach is to average the input vectors before applying them