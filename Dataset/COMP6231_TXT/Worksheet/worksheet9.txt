COMP 6721 Applied Artificial Intelligence (Fall 2023)Worksheet #9: Introduction to Natural Language Processing (NLP)Language Model. In Natural Language Processing (NLP), a bigram language model is a simple yet effective wayto understand the probability of a word sequence. It calculates the likelihood of a word wn appearing after a givenword wn−1. We use Maximum Likelihood Estimation (MLE) to determine these probabilities from a given corpus:P (wn|wn−1) = C(wn−1wn)C(wn−1), where C(wn−1wn) is the count of the occurrence of wn−1 followed by wn. So, given thefollowing corpus of three sentences:<s> I am Sam </s><s> Sam I am </s><s> I do not like green eggs and ham </s>compute the following bigram probabilities:P (I|<s>) = P (Sam|<s>) = P (am|I) =P (</s>|Sam) = P (Sam|am) = P (do|I) =Sentence Probability. Given an English language model with the following bigram probabilities, compute theprobability for the sentence “I want to eat British food”:P(on|eat) = .16 P(want|I) = .32 P(eat|to) = .26P(some|eat) = .06 P(would|I) = .29 P(have|to) = .14P(British|eat) = .001 P(don’t|I) = .08 P(spend|to) = .09... ... ...P(I|<s>) = .25 P(to|want) = .65 P(food|British) = .6P(I’d|<s>) = .06 P(a|want) = .5 P(restaurant|British) = .15P(</s> |British) = .1 P(</s> |food) = .25 P(</s> |restaurant) = .35P (I want to eat British food)===Corpus Probabilities. Given a corpus with |V | = 1616 different words and a total of N = 10000 bigrams:compute the probabilites for P (II) = , P (I|I) = and P (lunch|I) = .Smoothing. We can avoid zero probabilities by smoothing, here we use add-one (or Laplace) smoothing:computing the new bigram probabilities as PAdd1(wn|wn−1) = C(wn−1wn)+1C(wn−1)+|V | and PAdd1(wn−1wn) = C(wn−1wn)+1N+B ,where B is the number of “bins” we added +1 to (so here, |V |2). Compute the new probabilities:P (II) = , P (I|I) = and P (lunch|I) = .COMP6721 Worksheet: Introduction to NLP Fall 2023Part-of-Speech Tagging. Given the following lexicon, assign a part-of-speech (POS) tag to each word for thesentence below:I prefer a direct flight to Chigaco.Parsing. Now, given the following context-free grammar:create a parse tree for the sentence, “I prefer a direct flight to Chicago.” using the POS tags you assigned above:Word Sense Disambiguation. Using the following probabilities you obtained from a training corpus (|V |=50):Using “add 0.5” smoothing as shown above, with a context window of ±3, find the correct sense for bank in thesentence, “I like the Potomac bank”:1. Score(BANK1) =2. Score(BANK2) =Note: Words not shown in the list above have an unsmoothed probability of 0. Use logs.